

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>gensim.models.word2vec</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="" href="../../../index.html"/>
        <link rel="up" title="gensim.models" href="../models.html"/> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> NLP APIs
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Table of Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../gensim_tutorial/tutorial.html">Gensim Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nltk_intro.html">Natural Language Toolkit</a></li>
</ul>
<p class="caption"><span class="caption-text">Autogenerated API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/nltk.html"><code class="docutils literal"><span class="pre">nltk</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.html"><code class="docutils literal"><span class="pre">gensim</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.interfaces.html"><code class="docutils literal"><span class="pre">gensim.interfaces</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.matutils.html"><code class="docutils literal"><span class="pre">gensim.matutils</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.utils.html"><code class="docutils literal"><span class="pre">gensim.utils</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.corpora.html"><code class="docutils literal"><span class="pre">gensim.corpora</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.models.html"><code class="docutils literal"><span class="pre">gensim.models</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.parsing.html"><code class="docutils literal"><span class="pre">gensim.parsing</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.scripts.html"><code class="docutils literal"><span class="pre">gensim.scripts</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.similarities.html"><code class="docutils literal"><span class="pre">gensim.similarities</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.summarization.html"><code class="docutils literal"><span class="pre">gensim.summarization</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/gensim.topic_coherence.html"><code class="docutils literal"><span class="pre">gensim.topic_coherence</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/simserver.html"><code class="docutils literal"><span class="pre">simserver</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/word2vec.html"><code class="docutils literal"><span class="pre">word2vec</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/stop_words.html"><code class="docutils literal"><span class="pre">stop_words</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/nltk.classify.html"><code class="docutils literal"><span class="pre">nltk.classify</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../generated/nltk.cluster.html"><code class="docutils literal"><span class="pre">nltk.cluster</span></code></a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">NLP APIs</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          













<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
          <li><a href="../../gensim.html">gensim</a> &raquo;</li>
        
          <li><a href="../models.html">gensim.models</a> &raquo;</li>
        
      <li>gensim.models.word2vec</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for gensim.models.word2vec</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="c1">#</span>
<span class="c1"># Copyright (C) 2013 Radim Rehurek &lt;me@radimrehurek.com&gt;</span>
<span class="c1"># Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Deep learning via word2vec&#39;s &quot;skip-gram and CBOW models&quot;, using either</span>
<span class="sd">hierarchical softmax or negative sampling [1]_ [2]_.</span>

<span class="sd">The training algorithms were originally ported from the C package https://code.google.com/p/word2vec/</span>
<span class="sd">and extended with additional functionality.</span>

<span class="sd">For a blog tutorial on gensim word2vec, with an interactive web app trained on GoogleNews, visit http://radimrehurek.com/2014/02/word2vec-tutorial/</span>

<span class="sd">**Make sure you have a C compiler before installing gensim, to use optimized (compiled) word2vec training**</span>
<span class="sd">(70x speedup compared to plain NumPy implementation [3]_).</span>

<span class="sd">Initialize a model with e.g.::</span>

<span class="sd">&gt;&gt;&gt; model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)</span>

<span class="sd">Persist a model to disk with::</span>

<span class="sd">&gt;&gt;&gt; model.save(fname)</span>
<span class="sd">&gt;&gt;&gt; model = Word2Vec.load(fname)  # you can continue training with the loaded model!</span>

<span class="sd">The model can also be instantiated from an existing file on disk in the word2vec C format::</span>

<span class="sd">  &gt;&gt;&gt; model = Word2Vec.load_word2vec_format(&#39;/tmp/vectors.txt&#39;, binary=False)  # C text format</span>
<span class="sd">  &gt;&gt;&gt; model = Word2Vec.load_word2vec_format(&#39;/tmp/vectors.bin&#39;, binary=True)  # C binary format</span>

<span class="sd">You can perform various syntactic/semantic NLP word tasks with the model. Some of them</span>
<span class="sd">are already built-in::</span>

<span class="sd">  &gt;&gt;&gt; model.most_similar(positive=[&#39;woman&#39;, &#39;king&#39;], negative=[&#39;man&#39;])</span>
<span class="sd">  [(&#39;queen&#39;, 0.50882536), ...]</span>

<span class="sd">  &gt;&gt;&gt; model.doesnt_match(&quot;breakfast cereal dinner lunch&quot;.split())</span>
<span class="sd">  &#39;cereal&#39;</span>

<span class="sd">  &gt;&gt;&gt; model.similarity(&#39;woman&#39;, &#39;man&#39;)</span>
<span class="sd">  0.73723527</span>

<span class="sd">  &gt;&gt;&gt; model[&#39;computer&#39;]  # raw numpy vector of a word</span>
<span class="sd">  array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)</span>

<span class="sd">and so on.</span>

<span class="sd">If you&#39;re finished training a model (=no more updates, only querying), you can do</span>

<span class="sd">  &gt;&gt;&gt; model.init_sims(replace=True)</span>

<span class="sd">to trim unneeded model memory = use (much) less RAM.</span>

<span class="sd">Note that there is a :mod:`gensim.models.phrases` module which lets you automatically</span>
<span class="sd">detect phrases longer than one word. Using phrases, you can learn a word2vec model</span>
<span class="sd">where &quot;words&quot; are actually multiword expressions, such as `new_york_times` or `financial_crisis`:</span>

<span class="sd">&gt;&gt;&gt; bigram_transformer = gensim.models.Phrases(sentences)</span>
<span class="sd">&gt;&gt;&gt; model = Word2Vec(bigram_transformer[sentences], size=100, ...)</span>

<span class="sd">.. [1] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.</span>
<span class="sd">.. [2] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality.</span>
<span class="sd">       In Proceedings of NIPS, 2013.</span>
<span class="sd">.. [3] Optimizing word2vec in gensim, http://radimrehurek.com/2013/09/word2vec-in-python-part-two-optimizing/</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>  <span class="c1"># py3 &quot;true division&quot;</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">heapq</span>
<span class="kn">from</span> <span class="nn">timeit</span> <span class="kn">import</span> <span class="n">default_timer</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">itertools</span>

<span class="kn">from</span> <span class="nn">gensim.utils</span> <span class="kn">import</span> <span class="n">keep_vocab_item</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">queue</span> <span class="kn">import</span> <span class="n">Queue</span><span class="p">,</span> <span class="n">Empty</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">Queue</span> <span class="kn">import</span> <span class="n">Queue</span><span class="p">,</span> <span class="n">Empty</span>

<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">exp</span><span class="p">,</span> <span class="n">log</span><span class="p">,</span> <span class="n">dot</span><span class="p">,</span> <span class="n">zeros</span><span class="p">,</span> <span class="n">outer</span><span class="p">,</span> <span class="n">random</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">float32</span> <span class="k">as</span> <span class="n">REAL</span><span class="p">,</span>\
    <span class="n">double</span><span class="p">,</span> <span class="n">uint32</span><span class="p">,</span> <span class="n">seterr</span><span class="p">,</span> <span class="n">array</span><span class="p">,</span> <span class="n">uint8</span><span class="p">,</span> <span class="n">vstack</span><span class="p">,</span> <span class="n">fromstring</span><span class="p">,</span> <span class="n">sqrt</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">,</span>\
    <span class="n">ndarray</span><span class="p">,</span> <span class="n">empty</span><span class="p">,</span> <span class="nb">sum</span> <span class="k">as</span> <span class="n">np_sum</span><span class="p">,</span> <span class="n">prod</span><span class="p">,</span> <span class="n">ones</span><span class="p">,</span> <span class="n">ascontiguousarray</span>

<span class="kn">from</span> <span class="nn">gensim</span> <span class="kn">import</span> <span class="n">utils</span><span class="p">,</span> <span class="n">matutils</span>  <span class="c1"># utility fnc for pickling, common scipy operations etc</span>
<span class="kn">from</span> <span class="nn">gensim.corpora.dictionary</span> <span class="kn">import</span> <span class="n">Dictionary</span>
<span class="kn">from</span> <span class="nn">six</span> <span class="kn">import</span> <span class="n">iteritems</span><span class="p">,</span> <span class="n">itervalues</span><span class="p">,</span> <span class="n">string_types</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="nb">xrange</span>
<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">GeneratorType</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">gensim.models.word2vec_inner</span> <span class="kn">import</span> <span class="n">train_batch_sg</span><span class="p">,</span> <span class="n">train_batch_cbow</span>
    <span class="kn">from</span> <span class="nn">gensim.models.word2vec_inner</span> <span class="kn">import</span> <span class="n">score_sentence_sg</span><span class="p">,</span> <span class="n">score_sentence_cbow</span>
    <span class="kn">from</span> <span class="nn">gensim.models.word2vec_inner</span> <span class="kn">import</span> <span class="n">FAST_VERSION</span><span class="p">,</span> <span class="n">MAX_WORDS_IN_BATCH</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;Fast version of {0} is being used&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">__name__</span><span class="p">))</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="c1"># failed... fall back to plain numpy (20-80x slower training than the above)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;Slow version of {0} is being used&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">__name__</span><span class="p">))</span>
    <span class="n">FAST_VERSION</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">MAX_WORDS_IN_BATCH</span> <span class="o">=</span> <span class="mi">10000</span>

    <span class="k">def</span> <span class="nf">train_batch_sg</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">work</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update skip-gram model by training on a sequence of sentences.</span>

<span class="sd">        Each sentence is a list of string tokens, which are looked up in the model&#39;s</span>
<span class="sd">        vocab dictionary. Called internally from `Word2Vec.train()`.</span>

<span class="sd">        This is the non-optimized, Python version. If you have cython installed, gensim</span>
<span class="sd">        will use the optimized version from word2vec_inner instead.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
            <span class="n">word_vocabs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">vocab</span> <span class="ow">and</span>
                           <span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">.</span><span class="n">sample_int</span> <span class="o">&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">):</span>
                <span class="n">reduced_window</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">window</span><span class="p">)</span>  <span class="c1"># `b` in the original word2vec code</span>

                <span class="c1"># now go over all words from the (reduced) window, predicting each one in turn</span>
                <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pos</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="n">reduced_window</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">pos2</span><span class="p">,</span> <span class="n">word2</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">[</span><span class="n">start</span><span class="p">:(</span><span class="n">pos</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">reduced_window</span><span class="p">)],</span> <span class="n">start</span><span class="p">):</span>
                    <span class="c1"># don&#39;t train on the `word` itself</span>
                    <span class="k">if</span> <span class="n">pos2</span> <span class="o">!=</span> <span class="n">pos</span><span class="p">:</span>
                        <span class="n">train_sg_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">index</span><span class="p">],</span> <span class="n">word2</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">train_batch_cbow</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">work</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">neu1</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update CBOW model by training on a sequence of sentences.</span>

<span class="sd">        Each sentence is a list of string tokens, which are looked up in the model&#39;s</span>
<span class="sd">        vocab dictionary. Called internally from `Word2Vec.train()`.</span>

<span class="sd">        This is the non-optimized, Python version. If you have cython installed, gensim</span>
<span class="sd">        will use the optimized version from word2vec_inner instead.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
            <span class="n">word_vocabs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">vocab</span> <span class="ow">and</span>
                           <span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">.</span><span class="n">sample_int</span> <span class="o">&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">):</span>
                <span class="n">reduced_window</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">window</span><span class="p">)</span>  <span class="c1"># `b` in the original word2vec code</span>
                <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pos</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="n">reduced_window</span><span class="p">)</span>
                <span class="n">window_pos</span> <span class="o">=</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">[</span><span class="n">start</span><span class="p">:(</span><span class="n">pos</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">reduced_window</span><span class="p">)],</span> <span class="n">start</span><span class="p">)</span>
                <span class="n">word2_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2</span><span class="o">.</span><span class="n">index</span> <span class="k">for</span> <span class="n">pos2</span><span class="p">,</span> <span class="n">word2</span> <span class="ow">in</span> <span class="n">window_pos</span> <span class="k">if</span> <span class="p">(</span><span class="n">word2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">pos2</span> <span class="o">!=</span> <span class="n">pos</span><span class="p">)]</span>
                <span class="n">l1</span> <span class="o">=</span> <span class="n">np_sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">word2_indices</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 1 x vector_size</span>
                <span class="k">if</span> <span class="n">word2_indices</span> <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">cbow_mean</span><span class="p">:</span>
                    <span class="n">l1</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2_indices</span><span class="p">)</span>
                <span class="n">train_cbow_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">word2_indices</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">score_sentence_sg</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">work</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Obtain likelihood score for a single sentence in a fitted skip-gram representaion.</span>

<span class="sd">        The sentence is a list of Vocab objects (or None, when the corresponding</span>
<span class="sd">        word is not in the vocabulary). Called internally from `Word2Vec.score()`.</span>

<span class="sd">        This is the non-optimized, Python version. If you have cython installed, gensim</span>
<span class="sd">        will use the optimized version from word2vec_inner instead.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">log_prob_sentence</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;scoring is only available for HS=True&quot;</span><span class="p">)</span>

        <span class="n">word_vocabs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># OOV word in the input sentence =&gt; skip</span>

            <span class="c1"># now go over all words from the window, predicting each one in turn</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pos</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">pos2</span><span class="p">,</span> <span class="n">word2</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">[</span><span class="n">start</span> <span class="p">:</span> <span class="n">pos</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">start</span><span class="p">):</span>
                <span class="c1"># don&#39;t train on OOV words and on the `word` itself</span>
                <span class="k">if</span> <span class="n">word2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">pos2</span> <span class="o">!=</span> <span class="n">pos</span><span class="p">:</span>
                    <span class="n">log_prob_sentence</span> <span class="o">+=</span> <span class="n">score_sg_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">word2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob_sentence</span>

    <span class="k">def</span> <span class="nf">score_sentence_cbow</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">work</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">neu1</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Obtain likelihood score for a single sentence in a fitted CBOW representaion.</span>

<span class="sd">        The sentence is a list of Vocab objects (or None, where the corresponding</span>
<span class="sd">        word is not in the vocabulary. Called internally from `Word2Vec.score()`.</span>

<span class="sd">        This is the non-optimized, Python version. If you have cython installed, gensim</span>
<span class="sd">        will use the optimized version from word2vec_inner instead.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">log_prob_sentence</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;scoring is only available for HS=True&quot;</span><span class="p">)</span>

        <span class="n">word_vocabs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">sentence</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">word</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">continue</span>  <span class="c1"># OOV word in the input sentence =&gt; skip</span>

            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pos</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span><span class="p">)</span>
            <span class="n">window_pos</span> <span class="o">=</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">word_vocabs</span><span class="p">[</span><span class="n">start</span><span class="p">:(</span><span class="n">pos</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="n">start</span><span class="p">)</span>
            <span class="n">word2_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word2</span><span class="o">.</span><span class="n">index</span> <span class="k">for</span> <span class="n">pos2</span><span class="p">,</span> <span class="n">word2</span> <span class="ow">in</span> <span class="n">window_pos</span> <span class="k">if</span> <span class="p">(</span><span class="n">word2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">pos2</span> <span class="o">!=</span> <span class="n">pos</span><span class="p">)]</span>
            <span class="n">l1</span> <span class="o">=</span> <span class="n">np_sum</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">word2_indices</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 1 x layer1_size</span>
            <span class="k">if</span> <span class="n">word2_indices</span> <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">cbow_mean</span><span class="p">:</span>
                <span class="n">l1</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2_indices</span><span class="p">)</span>
            <span class="n">log_prob_sentence</span> <span class="o">+=</span> <span class="n">score_cbow_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">word2_indices</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob_sentence</span>

<span class="c1"># If pyemd C extension is available, import it.</span>
<span class="c1"># If pyemd is attempted to be used, but isn&#39;t installed, ImportError will be raised.</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">pyemd</span> <span class="kn">import</span> <span class="n">emd</span>
    <span class="n">PYEMD_EXT</span> <span class="o">=</span> <span class="bp">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">PYEMD_EXT</span> <span class="o">=</span> <span class="bp">False</span>

<span class="k">def</span> <span class="nf">train_sg_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">context_index</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">learn_vectors</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">learn_hidden</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                  <span class="n">context_vectors</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">context_locks</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">context_vectors</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">context_vectors</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0</span>
    <span class="k">if</span> <span class="n">context_locks</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">context_locks</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0_lockf</span>

    <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="n">predict_word</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>  <span class="c1"># target word (NN output)</span>

    <span class="n">l1</span> <span class="o">=</span> <span class="n">context_vectors</span><span class="p">[</span><span class="n">context_index</span><span class="p">]</span>  <span class="c1"># input word (NN input/projection layer)</span>
    <span class="n">lock_factor</span> <span class="o">=</span> <span class="n">context_locks</span><span class="p">[</span><span class="n">context_index</span><span class="p">]</span>

    <span class="n">neu1e</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">l1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
        <span class="c1"># work on the entire tree at once, to push as much work into numpy&#39;s C routines as possible (performance)</span>
        <span class="n">l2a</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">predict_word</span><span class="o">.</span><span class="n">point</span><span class="p">])</span>  <span class="c1"># 2d matrix, codelen x layer1_size</span>
        <span class="n">fa</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2a</span><span class="o">.</span><span class="n">T</span><span class="p">)))</span>  <span class="c1"># propagate hidden -&gt; output</span>
        <span class="n">ga</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">predict_word</span><span class="o">.</span><span class="n">code</span> <span class="o">-</span> <span class="n">fa</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>  <span class="c1"># vector of error gradients multiplied by the learning rate</span>
        <span class="k">if</span> <span class="n">learn_hidden</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">predict_word</span><span class="o">.</span><span class="n">point</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outer</span><span class="p">(</span><span class="n">ga</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>  <span class="c1"># learn hidden -&gt; output</span>
        <span class="n">neu1e</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">ga</span><span class="p">,</span> <span class="n">l2a</span><span class="p">)</span>  <span class="c1"># save error</span>

    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
        <span class="c1"># use this word (label = 1) + `negative` other random words not from this sentence (label = 0)</span>
        <span class="n">word_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">predict_word</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_indices</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cum_table</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cum_table</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">predict_word</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
                <span class="n">word_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">l2b</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn1neg</span><span class="p">[</span><span class="n">word_indices</span><span class="p">]</span>  <span class="c1"># 2d matrix, k+1 x layer1_size</span>
        <span class="n">fb</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2b</span><span class="o">.</span><span class="n">T</span><span class="p">)))</span>  <span class="c1"># propagate hidden -&gt; output</span>
        <span class="n">gb</span> <span class="o">=</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">neg_labels</span> <span class="o">-</span> <span class="n">fb</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>  <span class="c1"># vector of error gradients multiplied by the learning rate</span>
        <span class="k">if</span> <span class="n">learn_hidden</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn1neg</span><span class="p">[</span><span class="n">word_indices</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outer</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>  <span class="c1"># learn hidden -&gt; output</span>
        <span class="n">neu1e</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">l2b</span><span class="p">)</span>  <span class="c1"># save error</span>

    <span class="k">if</span> <span class="n">learn_vectors</span><span class="p">:</span>
        <span class="n">l1</span> <span class="o">+=</span> <span class="n">neu1e</span> <span class="o">*</span> <span class="n">lock_factor</span>  <span class="c1"># learn input -&gt; hidden (mutates model.syn0[word2.index], if that is l1)</span>
    <span class="k">return</span> <span class="n">neu1e</span>


<span class="k">def</span> <span class="nf">train_cbow_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">input_word_indices</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">learn_vectors</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">learn_hidden</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">neu1e</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">l1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
        <span class="n">l2a</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">point</span><span class="p">]</span>  <span class="c1"># 2d matrix, codelen x layer1_size</span>
        <span class="n">fa</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2a</span><span class="o">.</span><span class="n">T</span><span class="p">)))</span>  <span class="c1"># propagate hidden -&gt; output</span>
        <span class="n">ga</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">word</span><span class="o">.</span><span class="n">code</span> <span class="o">-</span> <span class="n">fa</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>  <span class="c1"># vector of error gradients multiplied by the learning rate</span>
        <span class="k">if</span> <span class="n">learn_hidden</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">point</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outer</span><span class="p">(</span><span class="n">ga</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>  <span class="c1"># learn hidden -&gt; output</span>
        <span class="n">neu1e</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">ga</span><span class="p">,</span> <span class="n">l2a</span><span class="p">)</span>  <span class="c1"># save error</span>

    <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
        <span class="c1"># use this word (label = 1) + `negative` other random words not from this sentence (label = 0)</span>
        <span class="n">word_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_indices</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cum_table</span><span class="o">.</span><span class="n">searchsorted</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">cum_table</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="n">word</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
                <span class="n">word_indices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">l2b</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn1neg</span><span class="p">[</span><span class="n">word_indices</span><span class="p">]</span>  <span class="c1"># 2d matrix, k+1 x layer1_size</span>
        <span class="n">fb</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2b</span><span class="o">.</span><span class="n">T</span><span class="p">)))</span>  <span class="c1"># propagate hidden -&gt; output</span>
        <span class="n">gb</span> <span class="o">=</span> <span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">neg_labels</span> <span class="o">-</span> <span class="n">fb</span><span class="p">)</span> <span class="o">*</span> <span class="n">alpha</span>  <span class="c1"># vector of error gradients multiplied by the learning rate</span>
        <span class="k">if</span> <span class="n">learn_hidden</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn1neg</span><span class="p">[</span><span class="n">word_indices</span><span class="p">]</span> <span class="o">+=</span> <span class="n">outer</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">l1</span><span class="p">)</span>  <span class="c1"># learn hidden -&gt; output</span>
        <span class="n">neu1e</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">gb</span><span class="p">,</span> <span class="n">l2b</span><span class="p">)</span>  <span class="c1"># save error</span>

    <span class="k">if</span> <span class="n">learn_vectors</span><span class="p">:</span>
        <span class="c1"># learn input -&gt; hidden, here for all words in the window separately</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">model</span><span class="o">.</span><span class="n">cbow_mean</span> <span class="ow">and</span> <span class="n">input_word_indices</span><span class="p">:</span>
            <span class="n">neu1e</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_word_indices</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">input_word_indices</span><span class="p">:</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">neu1e</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0_lockf</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">neu1e</span>


<span class="k">def</span> <span class="nf">score_sg_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">word2</span><span class="p">):</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">word2</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
    <span class="n">l2a</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">point</span><span class="p">])</span>  <span class="c1"># 2d matrix, codelen x layer1_size</span>
    <span class="n">sgn</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="o">**</span><span class="n">word</span><span class="o">.</span><span class="n">code</span>  <span class="c1"># ch function, 0-&gt; 1, 1 -&gt; -1</span>
    <span class="n">lprob</span> <span class="o">=</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">sgn</span><span class="o">*</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2a</span><span class="o">.</span><span class="n">T</span><span class="p">)))</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">lprob</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">score_cbow_pair</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">word2_indices</span><span class="p">,</span> <span class="n">l1</span><span class="p">):</span>
    <span class="n">l2a</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">syn1</span><span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">point</span><span class="p">]</span>  <span class="c1"># 2d matrix, codelen x layer1_size</span>
    <span class="n">sgn</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="o">**</span><span class="n">word</span><span class="o">.</span><span class="n">code</span>  <span class="c1"># ch function, 0-&gt; 1, 1 -&gt; -1</span>
    <span class="n">lprob</span> <span class="o">=</span> <span class="o">-</span><span class="n">log</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">sgn</span><span class="o">*</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2a</span><span class="o">.</span><span class="n">T</span><span class="p">)))</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">lprob</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Vocab</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A single vocabulary item, used internally for collecting per-word frequency/sampling info,</span>
<span class="sd">    and for constructing binary trees (incl. both word leaves and inner nodes).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__dict__</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__lt__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>  <span class="c1"># used for sorting in a priority queue</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">count</span> <span class="o">&lt;</span> <span class="n">other</span><span class="o">.</span><span class="n">count</span>

    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">vals</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;</span><span class="si">%s</span><span class="s1">:</span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__dict__</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__dict__</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)]</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">(</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">,</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">vals</span><span class="p">))</span>


<div class="viewcode-block" id="Word2Vec"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.html#gensim.models.Word2Vec">[docs]</a><span class="k">class</span> <span class="nc">Word2Vec</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">SaveLoad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Class for training, using and evaluating neural networks described in https://code.google.com/p/word2vec/</span>

<span class="sd">    The model can be stored/loaded via its `save()` and `load()` methods, or stored/loaded in a format</span>
<span class="sd">    compatible with the original word2vec implementation via `save_word2vec_format()` and `load_word2vec_format()`.</span>

<span class="sd">    &quot;&quot;&quot;</span>
<div class="viewcode-block" id="Word2Vec.__init__"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.__init__.html#gensim.models.Word2Vec.__init__">[docs]</a>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">max_vocab_size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">min_alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
            <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cbow_mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hashfxn</span><span class="o">=</span><span class="nb">hash</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">null_word</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">trim_rule</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">sorted_vocab</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_words</span><span class="o">=</span><span class="n">MAX_WORDS_IN_BATCH</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the model from an iterable of `sentences`. Each sentence is a</span>
<span class="sd">        list of words (unicode strings) that will be used for training.</span>

<span class="sd">        The `sentences` iterable can be simply a list, but for larger corpora,</span>
<span class="sd">        consider an iterable that streams the sentences directly from disk/network.</span>
<span class="sd">        See :class:`BrownCorpus`, :class:`Text8Corpus` or :class:`LineSentence` in</span>
<span class="sd">        this module for such examples.</span>

<span class="sd">        If you don&#39;t supply `sentences`, the model is left uninitialized -- use if</span>
<span class="sd">        you plan to initialize it in some other way.</span>

<span class="sd">        `sg` defines the training algorithm. By default (`sg=0`), CBOW is used.</span>
<span class="sd">        Otherwise (`sg=1`), skip-gram is employed.</span>

<span class="sd">        `size` is the dimensionality of the feature vectors.</span>

<span class="sd">        `window` is the maximum distance between the current and predicted word within a sentence.</span>

<span class="sd">        `alpha` is the initial learning rate (will linearly drop to `min_alpha` as training progresses).</span>

<span class="sd">        `seed` = for the random number generator. Initial vectors for each</span>
<span class="sd">        word are seeded with a hash of the concatenation of word + str(seed).</span>
<span class="sd">        Note that for a fully deterministically-reproducible run, you must also limit the model to</span>
<span class="sd">        a single worker thread, to eliminate ordering jitter from OS thread scheduling. (In Python</span>
<span class="sd">        3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED</span>
<span class="sd">        environment variable to control hash randomization.)</span>

<span class="sd">        `min_count` = ignore all words with total frequency lower than this.</span>

<span class="sd">        `max_vocab_size` = limit RAM during vocabulary building; if there are more unique</span>
<span class="sd">        words than this, then prune the infrequent ones. Every 10 million word types</span>
<span class="sd">        need about 1GB of RAM. Set to `None` for no limit (default).</span>

<span class="sd">        `sample` = threshold for configuring which higher-frequency words are randomly downsampled;</span>
<span class="sd">            default is 1e-3, useful range is (0, 1e-5).</span>

<span class="sd">        `workers` = use this many worker threads to train the model (=faster training with multicore machines).</span>

<span class="sd">        `hs` = if 1, hierarchical softmax will be used for model training.</span>
<span class="sd">        If set to 0 (default), and `negative` is non-zero, negative sampling will be used.</span>

<span class="sd">        `negative` = if &gt; 0, negative sampling will be used, the int for negative</span>
<span class="sd">        specifies how many &quot;noise words&quot; should be drawn (usually between 5-20).</span>
<span class="sd">        Default is 5. If set to 0, no negative samping is used.</span>

<span class="sd">        `cbow_mean` = if 0, use the sum of the context word vectors. If 1 (default), use the mean.</span>
<span class="sd">        Only applies when cbow is used.</span>

<span class="sd">        `hashfxn` = hash function to use to randomly initialize weights, for increased</span>
<span class="sd">        training reproducibility. Default is Python&#39;s rudimentary built in hash function.</span>

<span class="sd">        `iter` = number of iterations (epochs) over the corpus. Default is 5.</span>

<span class="sd">        `trim_rule` = vocabulary trimming rule, specifies whether certain words should remain</span>
<span class="sd">        in the vocabulary, be trimmed away, or handled using the default (discard if word count &lt; min_count).</span>
<span class="sd">        Can be None (min_count will be used), or a callable that accepts parameters (word, count, min_count) and</span>
<span class="sd">        returns either `utils.RULE_DISCARD`, `utils.RULE_KEEP` or `utils.RULE_DEFAULT`.</span>
<span class="sd">        Note: The rule, if given, is only used prune vocabulary during build_vocab() and is not stored as part</span>
<span class="sd">        of the model.</span>

<span class="sd">        `sorted_vocab` = if 1 (default), sort the vocabulary by descending frequency before</span>
<span class="sd">        assigning word indexes.</span>

<span class="sd">        `batch_words` = target size (in words) for batches of examples passed to worker threads (and</span>
<span class="sd">        thus cython routines). Default is 10000. (Larger batches will be passed if individual</span>
<span class="sd">        texts are longer than 10000 words, but the standard cython code truncates to that maximum.)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># mapping from a word (string) to a Vocab object</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># map from a word&#39;s matrix index (int) to word (string)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sg</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c1"># for negative sampling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">size</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;consider setting layer size to a multiple of 4 for greater performance&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha_yet_reached</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="c1"># To warn user if alpha increases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">window</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_vocab_size</span> <span class="o">=</span> <span class="n">max_vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample</span> <span class="o">=</span> <span class="n">sample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">workers</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">workers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">min_alpha</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hs</span> <span class="o">=</span> <span class="n">hs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">negative</span> <span class="o">=</span> <span class="n">negative</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cbow_mean</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cbow_mean</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hashfxn</span> <span class="o">=</span> <span class="n">hashfxn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter</span> <span class="o">=</span> <span class="nb">iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">null_word</span> <span class="o">=</span> <span class="n">null_word</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_train_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sorted_vocab</span> <span class="o">=</span> <span class="n">sorted_vocab</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_words</span> <span class="o">=</span> <span class="n">batch_words</span>

        <span class="k">if</span> <span class="n">sentences</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">GeneratorType</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;You can&#39;t pass a generator as the sentences argument. Try an iterator.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span></div>

<div class="viewcode-block" id="Word2Vec.make_cum_table"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.make_cum_table.html#gensim.models.Word2Vec.make_cum_table">[docs]</a>    <span class="k">def</span> <span class="nf">make_cum_table</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="mi">31</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a cumulative-distribution table using stored vocabulary word counts for</span>
<span class="sd">        drawing random words in the negative-sampling training routines.</span>

<span class="sd">        To draw a word index, choose a random integer up to the maximum value in the</span>
<span class="sd">        table (cum_table[-1]), then finding that integer&#39;s sorted insertion point</span>
<span class="sd">        (as if by bisect_left or ndarray.searchsorted()). That insertion point is the</span>
<span class="sd">        drawn index, coming up in proportion equal to the increment at that slot.</span>

<span class="sd">        Called internally from &#39;build_vocab()&#39;.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint32</span><span class="p">)</span>
        <span class="c1"># compute sum of all power (Z in paper)</span>
        <span class="n">train_words_pow</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="nb">sum</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="o">**</span><span class="n">power</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">]))</span>
        <span class="n">cumulative</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">word_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
            <span class="n">cumulative</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">word_index</span><span class="p">]]</span><span class="o">.</span><span class="n">count</span><span class="o">**</span><span class="n">power</span> <span class="o">/</span> <span class="n">train_words_pow</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span><span class="p">[</span><span class="n">word_index</span><span class="p">]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">cumulative</span> <span class="o">*</span> <span class="n">domain</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">domain</span></div>

<div class="viewcode-block" id="Word2Vec.create_binary_tree"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.create_binary_tree.html#gensim.models.Word2Vec.create_binary_tree">[docs]</a>    <span class="k">def</span> <span class="nf">create_binary_tree</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create a binary Huffman tree using stored vocabulary word counts. Frequent words</span>
<span class="sd">        will have shorter binary codes. Called internally from `build_vocab()`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;constructing a huffman tree from </span><span class="si">%i</span><span class="s2"> words&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">))</span>

        <span class="c1"># build the huffman tree</span>
        <span class="n">heap</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itervalues</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">))</span>
        <span class="n">heapq</span><span class="o">.</span><span class="n">heapify</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">min1</span><span class="p">,</span> <span class="n">min2</span> <span class="o">=</span> <span class="n">heapq</span><span class="o">.</span><span class="n">heappop</span><span class="p">(</span><span class="n">heap</span><span class="p">),</span> <span class="n">heapq</span><span class="o">.</span><span class="n">heappop</span><span class="p">(</span><span class="n">heap</span><span class="p">)</span>
            <span class="n">heapq</span><span class="o">.</span><span class="n">heappush</span><span class="p">(</span><span class="n">heap</span><span class="p">,</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="n">min1</span><span class="o">.</span><span class="n">count</span> <span class="o">+</span> <span class="n">min2</span><span class="o">.</span><span class="n">count</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">i</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">left</span><span class="o">=</span><span class="n">min1</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="n">min2</span><span class="p">))</span>

        <span class="c1"># recurse over the tree, assigning a binary code to each vocabulary word</span>
        <span class="k">if</span> <span class="n">heap</span><span class="p">:</span>
            <span class="n">max_depth</span><span class="p">,</span> <span class="n">stack</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="p">[(</span><span class="n">heap</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[],</span> <span class="p">[])]</span>
            <span class="k">while</span> <span class="n">stack</span><span class="p">:</span>
                <span class="n">node</span><span class="p">,</span> <span class="n">codes</span><span class="p">,</span> <span class="n">points</span> <span class="o">=</span> <span class="n">stack</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">):</span>
                    <span class="c1"># leaf node =&gt; store its path from the root</span>
                    <span class="n">node</span><span class="o">.</span><span class="n">code</span><span class="p">,</span> <span class="n">node</span><span class="o">.</span><span class="n">point</span> <span class="o">=</span> <span class="n">codes</span><span class="p">,</span> <span class="n">points</span>
                    <span class="n">max_depth</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">codes</span><span class="p">),</span> <span class="n">max_depth</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># inner node =&gt; continue recursion</span>
                    <span class="n">points</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">points</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">index</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint32</span><span class="p">)</span>
                    <span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">node</span><span class="o">.</span><span class="n">left</span><span class="p">,</span> <span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">codes</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint8</span><span class="p">),</span> <span class="n">points</span><span class="p">))</span>
                    <span class="n">stack</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">node</span><span class="o">.</span><span class="n">right</span><span class="p">,</span> <span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">codes</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint8</span><span class="p">),</span> <span class="n">points</span><span class="p">))</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;built huffman tree with maximum node depth </span><span class="si">%i</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">)</span></div>

<div class="viewcode-block" id="Word2Vec.build_vocab"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.build_vocab.html#gensim.models.Word2Vec.build_vocab">[docs]</a>    <span class="k">def</span> <span class="nf">build_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">keep_raw_vocab</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">progress_per</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Build vocabulary from a sequence of sentences (can be a once-only generator stream).</span>
<span class="sd">        Each sentence must be a list of unicode strings.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scan_vocab</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">progress_per</span><span class="o">=</span><span class="n">progress_per</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">)</span>  <span class="c1"># initial survey</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale_vocab</span><span class="p">(</span><span class="n">keep_raw_vocab</span><span class="o">=</span><span class="n">keep_raw_vocab</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">)</span>  <span class="c1"># trim by min_count &amp; precalculate downsampling</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">finalize_vocab</span><span class="p">()</span>  <span class="c1"># build tables &amp; arrays</span></div>

<div class="viewcode-block" id="Word2Vec.scan_vocab"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.scan_vocab.html#gensim.models.Word2Vec.scan_vocab">[docs]</a>    <span class="k">def</span> <span class="nf">scan_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">progress_per</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Do an initial scan of all words appearing in sentences.&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;collecting all words and their counts&quot;</span><span class="p">)</span>
        <span class="n">sentence_no</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">total_words</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">min_reduce</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">checked_string_types</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">sentence_no</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">checked_string_types</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">string_types</span><span class="p">):</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Each &#39;sentences&#39; item should be a list of words (usually unicode strings).&quot;</span>
                                <span class="s2">&quot;First item here is instead plain </span><span class="si">%s</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">sentence</span><span class="p">))</span>
                <span class="n">checked_string_types</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">sentence_no</span> <span class="o">%</span> <span class="n">progress_per</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;PROGRESS: at sentence #</span><span class="si">%i</span><span class="s2">, processed </span><span class="si">%i</span><span class="s2"> words, keeping </span><span class="si">%i</span><span class="s2"> word types&quot;</span><span class="p">,</span>
                            <span class="n">sentence_no</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">itervalues</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span> <span class="o">+</span> <span class="n">total_words</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
                <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_vocab_size</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_vocab_size</span><span class="p">:</span>
                <span class="n">total_words</span> <span class="o">+=</span> <span class="n">utils</span><span class="o">.</span><span class="n">prune_vocab</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">min_reduce</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">)</span>
                <span class="n">min_reduce</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">total_words</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">itervalues</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;collected </span><span class="si">%i</span><span class="s2"> word types from a corpus of </span><span class="si">%i</span><span class="s2"> raw words and </span><span class="si">%i</span><span class="s2"> sentences&quot;</span><span class="p">,</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">total_words</span><span class="p">,</span> <span class="n">sentence_no</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_count</span> <span class="o">=</span> <span class="n">sentence_no</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span> <span class="o">=</span> <span class="n">vocab</span></div>

<div class="viewcode-block" id="Word2Vec.scale_vocab"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.scale_vocab.html#gensim.models.Word2Vec.scale_vocab">[docs]</a>    <span class="k">def</span> <span class="nf">scale_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dry_run</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">keep_raw_vocab</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply vocabulary settings for `min_count` (discarding less-frequent words)</span>
<span class="sd">        and `sample` (controlling the downsampling of more-frequent words).</span>

<span class="sd">        Calling with `dry_run=True` will only simulate the provided settings and</span>
<span class="sd">        report the size of the retained vocabulary, effective corpus length, and</span>
<span class="sd">        estimated memory requirements. Results are both printed via logging and</span>
<span class="sd">        returned as a dict.</span>

<span class="sd">        Delete the raw vocabulary after the scaling is done to free up RAM,</span>
<span class="sd">        unless `keep_raw_vocab` is set.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_count</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">sample</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span>

        <span class="c1"># Discard words less-frequent than min_count</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">dry_run</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="c1"># make stored settings match these applied settings</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_count</span> <span class="o">=</span> <span class="n">min_count</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sample</span> <span class="o">=</span> <span class="n">sample</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">drop_unique</span><span class="p">,</span> <span class="n">drop_total</span><span class="p">,</span> <span class="n">retain_total</span><span class="p">,</span> <span class="n">original_total</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="n">retain_words</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">keep_vocab_item</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">min_count</span><span class="p">,</span> <span class="n">trim_rule</span><span class="o">=</span><span class="n">trim_rule</span><span class="p">):</span>
                <span class="n">retain_words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
                <span class="n">retain_total</span> <span class="o">+=</span> <span class="n">v</span>
                <span class="n">original_total</span> <span class="o">+=</span> <span class="n">v</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">dry_run</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="n">v</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">drop_unique</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">drop_total</span> <span class="o">+=</span> <span class="n">v</span>
                <span class="n">original_total</span> <span class="o">+=</span> <span class="n">v</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;min_count=</span><span class="si">%d</span><span class="s2"> retains </span><span class="si">%i</span><span class="s2"> unique words (drops </span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span>
                    <span class="n">min_count</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">retain_words</span><span class="p">),</span> <span class="n">drop_unique</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;min_count leaves </span><span class="si">%i</span><span class="s2"> word corpus (</span><span class="si">%i%%</span><span class="s2"> of original </span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span>
                    <span class="n">retain_total</span><span class="p">,</span> <span class="n">retain_total</span> <span class="o">*</span> <span class="mi">100</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">original_total</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">original_total</span><span class="p">)</span>

        <span class="c1"># Precalculate each vocabulary item&#39;s threshold for sampling</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">sample</span><span class="p">:</span>
            <span class="c1"># no words downsampled</span>
            <span class="n">threshold_count</span> <span class="o">=</span> <span class="n">retain_total</span>
        <span class="k">elif</span> <span class="n">sample</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="c1"># traditional meaning: set parameter as proportion of total</span>
            <span class="n">threshold_count</span> <span class="o">=</span> <span class="n">sample</span> <span class="o">*</span> <span class="n">retain_total</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># new shorthand: sample &gt;= 1 means downsample all words with higher count than sample</span>
            <span class="n">threshold_count</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sample</span> <span class="o">*</span> <span class="p">(</span><span class="mi">3</span> <span class="o">+</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">downsample_total</span><span class="p">,</span> <span class="n">downsample_unique</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">retain_words</span><span class="p">:</span>
            <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
            <span class="n">word_probability</span> <span class="o">=</span> <span class="p">(</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span> <span class="o">/</span> <span class="n">threshold_count</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">threshold_count</span> <span class="o">/</span> <span class="n">v</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">word_probability</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="n">downsample_unique</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">downsample_total</span> <span class="o">+=</span> <span class="n">word_probability</span> <span class="o">*</span> <span class="n">v</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">word_probability</span> <span class="o">=</span> <span class="mf">1.0</span>
                <span class="n">downsample_total</span> <span class="o">+=</span> <span class="n">v</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">dry_run</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">.</span><span class="n">sample_int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">word_probability</span> <span class="o">*</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">dry_run</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">keep_raw_vocab</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;deleting the raw counts dictionary of </span><span class="si">%i</span><span class="s2"> items&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">raw_vocab</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;sample=</span><span class="si">%g</span><span class="s2"> downsamples </span><span class="si">%i</span><span class="s2"> most-common words&quot;</span><span class="p">,</span> <span class="n">sample</span><span class="p">,</span> <span class="n">downsample_unique</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;downsampling leaves estimated </span><span class="si">%i</span><span class="s2"> word corpus (</span><span class="si">%.1f%%</span><span class="s2"> of prior </span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span>
                    <span class="n">downsample_total</span><span class="p">,</span> <span class="n">downsample_total</span> <span class="o">*</span> <span class="mf">100.0</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="n">retain_total</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">retain_total</span><span class="p">)</span>

        <span class="c1"># return from each step: words-affected, resulting-corpus-size</span>
        <span class="n">report_values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;drop_unique&#39;</span><span class="p">:</span> <span class="n">drop_unique</span><span class="p">,</span> <span class="s1">&#39;retain_total&#39;</span><span class="p">:</span> <span class="n">retain_total</span><span class="p">,</span>
                         <span class="s1">&#39;downsample_unique&#39;</span><span class="p">:</span> <span class="n">downsample_unique</span><span class="p">,</span> <span class="s1">&#39;downsample_total&#39;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">downsample_total</span><span class="p">)}</span>

        <span class="c1"># print extra memory estimates</span>
        <span class="n">report_values</span><span class="p">[</span><span class="s1">&#39;memory&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimate_memory</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">retain_words</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">report_values</span></div>

<div class="viewcode-block" id="Word2Vec.finalize_vocab"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.finalize_vocab.html#gensim.models.Word2Vec.finalize_vocab">[docs]</a>    <span class="k">def</span> <span class="nf">finalize_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Build tables and model weights based on final vocabulary settings.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_vocab</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sorted_vocab</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sort_vocab</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
            <span class="c1"># add info about each word&#39;s Huffman encoding</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">create_binary_tree</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="c1"># build the table for drawing random words (for negative sampling)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_cum_table</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">null_word</span><span class="p">:</span>
            <span class="c1"># create null pseudo-word for padding when using concatenative L1 (run-of-words)</span>
            <span class="c1"># this word is only ever input – never predicted – so count, huffman-point, etc doesn&#39;t matter</span>
            <span class="n">word</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\0</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_int</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">v</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="c1"># set initial input/projection and hidden weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_weights</span><span class="p">()</span></div>

<div class="viewcode-block" id="Word2Vec.sort_vocab"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.sort_vocab.html#gensim.models.Word2Vec.sort_vocab">[docs]</a>    <span class="k">def</span> <span class="nf">sort_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sort the vocabulary so the most frequent words have the lowest indexes.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;syn0&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;must sort before initializing vectors/weights&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">i</span></div>

<div class="viewcode-block" id="Word2Vec.reset_from"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.reset_from.html#gensim.models.Word2Vec.reset_from">[docs]</a>    <span class="k">def</span> <span class="nf">reset_from</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other_model</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Borrow shareable pre-built structures (like vocab) from the other_model. Useful</span>
<span class="sd">        if testing multiple models in parallel on the same corpus.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">other_model</span><span class="o">.</span><span class="n">vocab</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span> <span class="o">=</span> <span class="n">other_model</span><span class="o">.</span><span class="n">index2word</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cum_table</span> <span class="o">=</span> <span class="n">other_model</span><span class="o">.</span><span class="n">cum_table</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_count</span> <span class="o">=</span> <span class="n">other_model</span><span class="o">.</span><span class="n">corpus_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_weights</span><span class="p">()</span></div>

    <span class="k">def</span> <span class="nf">_do_train_job</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">inits</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Train a single batch of sentences. Return 2-tuple `(effective word count after</span>
<span class="sd">        ignoring unknown words and sentence length trimming, total word count)`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">work</span><span class="p">,</span> <span class="n">neu1</span> <span class="o">=</span> <span class="n">inits</span>
        <span class="n">tally</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sg</span><span class="p">:</span>
            <span class="n">tally</span> <span class="o">+=</span> <span class="n">train_batch_sg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">work</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tally</span> <span class="o">+=</span> <span class="n">train_batch_cbow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">work</span><span class="p">,</span> <span class="n">neu1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tally</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_word_count</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_raw_word_count</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">job</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the number of words in a given job.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">job</span><span class="p">)</span>

<div class="viewcode-block" id="Word2Vec.train"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.train.html#gensim.models.Word2Vec.train">[docs]</a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">total_words</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">word_count</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
              <span class="n">total_examples</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">queue_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">report_delay</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update the model&#39;s neural weights from a sequence of sentences (can be a once-only generator stream).</span>
<span class="sd">        For Word2Vec, each sentence must be a list of unicode strings. (Subclasses may accept other examples.)</span>

<span class="sd">        To support linear learning-rate decay from (initial) alpha to min_alpha, either total_examples</span>
<span class="sd">        (count of sentences) or total_words (count of raw words in sentences) should be provided, unless the</span>
<span class="sd">        sentences are the same as those that were used to initially build the vocabulary.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">FAST_VERSION</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">warnings</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;C extension not loaded for Word2Vec, training will be slow. &quot;</span>
                          <span class="s2">&quot;Install a C compiler and reinstall gensim for fast training.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">neg_labels</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># precompute negative labels optimization for pure-python training</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">neg_labels</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">negative</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">neg_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;training model with </span><span class="si">%i</span><span class="s2"> workers on </span><span class="si">%i</span><span class="s2"> vocabulary and </span><span class="si">%i</span><span class="s2"> features, &quot;</span>
            <span class="s2">&quot;using sg=</span><span class="si">%s</span><span class="s2"> hs=</span><span class="si">%s</span><span class="s2"> sample=</span><span class="si">%s</span><span class="s2"> negative=</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sg</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;you must first build vocabulary before training the model&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;syn0&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;you must first finalize vocabulary before training the model&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">total_words</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">total_examples</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">:</span>
                <span class="n">total_examples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">corpus_count</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;expecting </span><span class="si">%i</span><span class="s2"> sentences, matching count from corpus used for vocabulary survey&quot;</span><span class="p">,</span> <span class="n">total_examples</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;you must provide either total_words or total_examples, to enable alpha and progress calculations&quot;</span><span class="p">)</span>

        <span class="n">job_tally</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">sentences</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">RepeatCorpusNTimes</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter</span><span class="p">)</span>
            <span class="n">total_words</span> <span class="o">=</span> <span class="n">total_words</span> <span class="ow">and</span> <span class="n">total_words</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter</span>
            <span class="n">total_examples</span> <span class="o">=</span> <span class="n">total_examples</span> <span class="ow">and</span> <span class="n">total_examples</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter</span>

        <span class="k">def</span> <span class="nf">worker_loop</span><span class="p">():</span>
            <span class="sd">&quot;&quot;&quot;Train the model, lifting lists of sentences from the job_queue.&quot;&quot;&quot;</span>
            <span class="n">work</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">zeros_aligned</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>  <span class="c1"># per-thread private work memory</span>
            <span class="n">neu1</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">zeros_aligned</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
            <span class="n">jobs_processed</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
                <span class="n">job</span> <span class="o">=</span> <span class="n">job_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">job</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="n">progress_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>
                    <span class="k">break</span>  <span class="c1"># no more jobs =&gt; quit this worker</span>
                <span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">job</span>
                <span class="n">tally</span><span class="p">,</span> <span class="n">raw_tally</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_do_train_job</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="p">(</span><span class="n">work</span><span class="p">,</span> <span class="n">neu1</span><span class="p">))</span>
                <span class="n">progress_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">tally</span><span class="p">,</span> <span class="n">raw_tally</span><span class="p">))</span>  <span class="c1"># report back progress</span>
                <span class="n">jobs_processed</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;worker exiting, processed </span><span class="si">%i</span><span class="s2"> jobs&quot;</span><span class="p">,</span> <span class="n">jobs_processed</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">job_producer</span><span class="p">():</span>
            <span class="sd">&quot;&quot;&quot;Fill jobs queue using the input `sentences` iterator.&quot;&quot;&quot;</span>
            <span class="n">job_batch</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="p">[],</span> <span class="mi">0</span>
            <span class="n">pushed_words</span><span class="p">,</span> <span class="n">pushed_examples</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
            <span class="n">next_alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
            <span class="k">if</span> <span class="n">next_alpha</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha_yet_reached</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Effective &#39;alpha&#39; higher than previous training cycles&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha_yet_reached</span> <span class="o">=</span> <span class="n">next_alpha</span>
            <span class="n">job_no</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">for</span> <span class="n">sent_idx</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentences</span><span class="p">):</span>
                <span class="n">sentence_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_word_count</span><span class="p">([</span><span class="n">sentence</span><span class="p">])</span>

                <span class="c1"># can we fit this sentence into the existing job batch?</span>
                <span class="k">if</span> <span class="n">batch_size</span> <span class="o">+</span> <span class="n">sentence_length</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_words</span><span class="p">:</span>
                    <span class="c1"># yes =&gt; add it to the current job</span>
                    <span class="n">job_batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
                    <span class="n">batch_size</span> <span class="o">+=</span> <span class="n">sentence_length</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># no =&gt; submit the existing job</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                        <span class="s2">&quot;queueing job #</span><span class="si">%i</span><span class="s2"> (</span><span class="si">%i</span><span class="s2"> words, </span><span class="si">%i</span><span class="s2"> sentences) at alpha </span><span class="si">%.05f</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="n">job_no</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">job_batch</span><span class="p">),</span> <span class="n">next_alpha</span><span class="p">)</span>
                    <span class="n">job_no</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">job_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">job_batch</span><span class="p">,</span> <span class="n">next_alpha</span><span class="p">))</span>

                    <span class="c1"># update the learning rate for the next job</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha</span> <span class="o">&lt;</span> <span class="n">next_alpha</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">total_examples</span><span class="p">:</span>
                            <span class="c1"># examples-based decay</span>
                            <span class="n">pushed_examples</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">job_batch</span><span class="p">)</span>
                            <span class="n">progress</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">pushed_examples</span> <span class="o">/</span> <span class="n">total_examples</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># words-based decay</span>
                            <span class="n">pushed_words</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_raw_word_count</span><span class="p">(</span><span class="n">job_batch</span><span class="p">)</span>
                            <span class="n">progress</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">pushed_words</span> <span class="o">/</span> <span class="n">total_words</span>
                        <span class="n">next_alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">progress</span>
                        <span class="n">next_alpha</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_alpha</span><span class="p">,</span> <span class="n">next_alpha</span><span class="p">)</span>

                    <span class="c1"># add the sentence that didn&#39;t fit as the first item of a new job</span>
                    <span class="n">job_batch</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence</span><span class="p">],</span> <span class="n">sentence_length</span>

            <span class="c1"># add the last job too (may be significantly smaller than batch_words)</span>
            <span class="k">if</span> <span class="n">job_batch</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                    <span class="s2">&quot;queueing job #</span><span class="si">%i</span><span class="s2"> (</span><span class="si">%i</span><span class="s2"> words, </span><span class="si">%i</span><span class="s2"> sentences) at alpha </span><span class="si">%.05f</span><span class="s2">&quot;</span><span class="p">,</span>
                    <span class="n">job_no</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">job_batch</span><span class="p">),</span> <span class="n">next_alpha</span><span class="p">)</span>
                <span class="n">job_no</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">job_queue</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">job_batch</span><span class="p">,</span> <span class="n">next_alpha</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">job_no</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;train() called with an empty iterator (if not intended, &quot;</span>
                    <span class="s2">&quot;be sure to provide a corpus that offers restartable &quot;</span>
                    <span class="s2">&quot;iteration = an iterable).&quot;</span>
                <span class="p">)</span>

            <span class="c1"># give the workers heads up that they can finish -- no more work!</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">):</span>
                <span class="n">job_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;job loop exiting, total </span><span class="si">%i</span><span class="s2"> jobs&quot;</span><span class="p">,</span> <span class="n">job_no</span><span class="p">)</span>

        <span class="c1"># buffer ahead only a limited number of jobs.. this is the reason we can&#39;t simply use ThreadPool :(</span>
        <span class="n">job_queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="n">queue_factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span>
        <span class="n">progress_queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="p">(</span><span class="n">queue_factor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span>

        <span class="n">workers</span> <span class="o">=</span> <span class="p">[</span><span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">worker_loop</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)]</span>
        <span class="n">unfinished_worker_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">workers</span><span class="p">)</span>
        <span class="n">workers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">job_producer</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="n">workers</span><span class="p">:</span>
            <span class="n">thread</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="bp">True</span>  <span class="c1"># make interrupting the process with ctrl+c easier</span>
            <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

        <span class="n">example_count</span><span class="p">,</span> <span class="n">trained_word_count</span><span class="p">,</span> <span class="n">raw_word_count</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">word_count</span>
        <span class="n">start</span><span class="p">,</span> <span class="n">next_report</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.00001</span><span class="p">,</span> <span class="mf">1.0</span>

        <span class="k">while</span> <span class="n">unfinished_worker_count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">report</span> <span class="o">=</span> <span class="n">progress_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>  <span class="c1"># blocks if workers too slow</span>
            <span class="k">if</span> <span class="n">report</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>  <span class="c1"># a thread reporting that it finished</span>
                <span class="n">unfinished_worker_count</span> <span class="o">-=</span> <span class="mi">1</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;worker thread finished; awaiting finish of </span><span class="si">%i</span><span class="s2"> more threads&quot;</span><span class="p">,</span> <span class="n">unfinished_worker_count</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="n">examples</span><span class="p">,</span> <span class="n">trained_words</span><span class="p">,</span> <span class="n">raw_words</span> <span class="o">=</span> <span class="n">report</span>
            <span class="n">job_tally</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># update progress stats</span>
            <span class="n">example_count</span> <span class="o">+=</span> <span class="n">examples</span>
            <span class="n">trained_word_count</span> <span class="o">+=</span> <span class="n">trained_words</span>  <span class="c1"># only words in vocab &amp; sampled</span>
            <span class="n">raw_word_count</span> <span class="o">+=</span> <span class="n">raw_words</span>

            <span class="c1"># log progress once every report_delay seconds</span>
            <span class="n">elapsed</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
            <span class="k">if</span> <span class="n">elapsed</span> <span class="o">&gt;=</span> <span class="n">next_report</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">total_examples</span><span class="p">:</span>
                    <span class="c1"># examples-based progress %</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;PROGRESS: at </span><span class="si">%.2f%%</span><span class="s2"> examples, </span><span class="si">%.0f</span><span class="s2"> words/s, in_qsize </span><span class="si">%i</span><span class="s2">, out_qsize </span><span class="si">%i</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="mf">100.0</span> <span class="o">*</span> <span class="n">example_count</span> <span class="o">/</span> <span class="n">total_examples</span><span class="p">,</span> <span class="n">trained_word_count</span> <span class="o">/</span> <span class="n">elapsed</span><span class="p">,</span>
                        <span class="n">utils</span><span class="o">.</span><span class="n">qsize</span><span class="p">(</span><span class="n">job_queue</span><span class="p">),</span> <span class="n">utils</span><span class="o">.</span><span class="n">qsize</span><span class="p">(</span><span class="n">progress_queue</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># words-based progress %</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="s2">&quot;PROGRESS: at </span><span class="si">%.2f%%</span><span class="s2"> words, </span><span class="si">%.0f</span><span class="s2"> words/s, in_qsize </span><span class="si">%i</span><span class="s2">, out_qsize </span><span class="si">%i</span><span class="s2">&quot;</span><span class="p">,</span>
                        <span class="mf">100.0</span> <span class="o">*</span> <span class="n">raw_word_count</span> <span class="o">/</span> <span class="n">total_words</span><span class="p">,</span> <span class="n">trained_word_count</span> <span class="o">/</span> <span class="n">elapsed</span><span class="p">,</span>
                        <span class="n">utils</span><span class="o">.</span><span class="n">qsize</span><span class="p">(</span><span class="n">job_queue</span><span class="p">),</span> <span class="n">utils</span><span class="o">.</span><span class="n">qsize</span><span class="p">(</span><span class="n">progress_queue</span><span class="p">))</span>
                <span class="n">next_report</span> <span class="o">=</span> <span class="n">elapsed</span> <span class="o">+</span> <span class="n">report_delay</span>

        <span class="c1"># all done; report the final stats</span>
        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;training on </span><span class="si">%i</span><span class="s2"> raw words (</span><span class="si">%i</span><span class="s2"> effective words) took </span><span class="si">%.1f</span><span class="s2">s, </span><span class="si">%.0f</span><span class="s2"> effective words/s&quot;</span><span class="p">,</span>
            <span class="n">raw_word_count</span><span class="p">,</span> <span class="n">trained_word_count</span><span class="p">,</span> <span class="n">elapsed</span><span class="p">,</span> <span class="n">trained_word_count</span> <span class="o">/</span> <span class="n">elapsed</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">job_tally</span> <span class="o">&lt;</span> <span class="mi">10</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;under 10 jobs per worker: consider setting a smaller `batch_words&#39; for smoother alpha decay&quot;</span><span class="p">)</span>

        <span class="c1"># check that the input corpus hasn&#39;t changed during iteration</span>
        <span class="k">if</span> <span class="n">total_examples</span> <span class="ow">and</span> <span class="n">total_examples</span> <span class="o">!=</span> <span class="n">example_count</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;supplied example count (</span><span class="si">%i</span><span class="s2">) did not equal expected count (</span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">example_count</span><span class="p">,</span> <span class="n">total_examples</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">total_words</span> <span class="ow">and</span> <span class="n">total_words</span> <span class="o">!=</span> <span class="n">raw_word_count</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;supplied raw word count (</span><span class="si">%i</span><span class="s2">) did not equal expected count (</span><span class="si">%i</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">raw_word_count</span><span class="p">,</span> <span class="n">total_words</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">train_count</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># number of times train() has been called</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_train_time</span> <span class="o">+=</span> <span class="n">elapsed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clear_sims</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">trained_word_count</span></div>

    <span class="c1"># basics copied from the train() function</span>
<div class="viewcode-block" id="Word2Vec.score"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.score.html#gensim.models.Word2Vec.score">[docs]</a>    <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentences</span><span class="p">,</span> <span class="n">total_sentences</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">),</span> <span class="n">chunksize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">queue_factor</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">report_delay</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Score the log probability for a sequence of sentences (can be a once-only generator stream).</span>
<span class="sd">        Each sentence must be a list of unicode strings.</span>
<span class="sd">        This does not change the fitted model in any way (see Word2Vec.train() for that).</span>

<span class="sd">        We have currently only implemented score for the hierarchical softmax scheme,</span>
<span class="sd">        so you need to have run word2vec with hs=1 and negative=0 for this to work.</span>

<span class="sd">        Note that you should specify total_sentences; we&#39;ll run into problems if you ask to</span>
<span class="sd">        score more than this number of sentences but it is inefficient to set the value too high.</span>

<span class="sd">        See the article by [taddy]_ and the gensim demo at [deepir]_ for examples of how to use such scores in document classification.</span>

<span class="sd">        .. [taddy] Taddy, Matt.  Document Classification by Inversion of Distributed Language Representations, in Proceedings of the 2015 Conference of the Association of Computational Linguistics.</span>
<span class="sd">        .. [deepir] https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">FAST_VERSION</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="kn">import</span> <span class="nn">warnings</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;C extension compilation failed, scoring will be slow. &quot;</span>
                          <span class="s2">&quot;Install a C compiler and reinstall gensim for fastness.&quot;</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;scoring sentences with </span><span class="si">%i</span><span class="s2"> workers on </span><span class="si">%i</span><span class="s2"> vocabulary and </span><span class="si">%i</span><span class="s2"> features, &quot;</span>
            <span class="s2">&quot;using sg=</span><span class="si">%s</span><span class="s2"> hs=</span><span class="si">%s</span><span class="s2"> sample=</span><span class="si">%s</span><span class="s2"> and negative=</span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sg</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;you must first build vocabulary before scoring new data&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;we have only implemented score for hs&quot;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">worker_loop</span><span class="p">():</span>
            <span class="sd">&quot;&quot;&quot;Train the model, lifting lists of sentences from the jobs queue.&quot;&quot;&quot;</span>
            <span class="n">work</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>  <span class="c1"># for sg hs, we actually only need one memory loc (running sum)</span>
            <span class="n">neu1</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">zeros_aligned</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
            <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
                <span class="n">job</span> <span class="o">=</span> <span class="n">job_queue</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">job</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>  <span class="c1"># signal to finish</span>
                    <span class="k">break</span>
                <span class="n">ns</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">sentence_id</span><span class="p">,</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">job</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">sentence_id</span> <span class="o">&gt;=</span> <span class="n">total_sentences</span><span class="p">:</span>
                        <span class="k">break</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sg</span><span class="p">:</span>
                        <span class="n">score</span> <span class="o">=</span> <span class="n">score_sentence_sg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">work</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">score</span> <span class="o">=</span> <span class="n">score_sentence_cbow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">,</span> <span class="n">work</span><span class="p">,</span> <span class="n">neu1</span><span class="p">)</span>
                    <span class="n">sentence_scores</span><span class="p">[</span><span class="n">sentence_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">score</span>
                    <span class="n">ns</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">progress_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">ns</span><span class="p">)</span>  <span class="c1"># report progress</span>

        <span class="n">start</span><span class="p">,</span> <span class="n">next_report</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">(),</span> <span class="mf">1.0</span>
        <span class="c1"># buffer ahead only a limited number of jobs.. this is the reason we can&#39;t simply use ThreadPool :(</span>
        <span class="n">job_queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="n">queue_factor</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span>
        <span class="n">progress_queue</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">(</span><span class="n">maxsize</span><span class="o">=</span><span class="p">(</span><span class="n">queue_factor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)</span>

        <span class="n">workers</span> <span class="o">=</span> <span class="p">[</span><span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">worker_loop</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">)]</span>
        <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="n">workers</span><span class="p">:</span>
            <span class="n">thread</span><span class="o">.</span><span class="n">daemon</span> <span class="o">=</span> <span class="bp">True</span>  <span class="c1"># make interrupting the process with ctrl+c easier</span>
            <span class="n">thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

        <span class="n">sentence_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">sentence_scores</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">zeros_aligned</span><span class="p">(</span><span class="n">total_sentences</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>

        <span class="n">push_done</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="n">done_jobs</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">jobs_source</span> <span class="o">=</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">grouper</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">sentences</span><span class="p">),</span> <span class="n">chunksize</span><span class="p">))</span>

        <span class="c1"># fill jobs queue with (id, sentence) job items</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">job_no</span><span class="p">,</span> <span class="n">items</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">jobs_source</span><span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">job_no</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunksize</span> <span class="o">&gt;</span> <span class="n">total_sentences</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;terminating after </span><span class="si">%i</span><span class="s2"> sentences (set higher total_sentences if you want more).&quot;</span><span class="p">,</span>
                        <span class="n">total_sentences</span><span class="p">)</span>
                    <span class="n">job_no</span> <span class="o">-=</span> <span class="mi">1</span>
                    <span class="k">raise</span> <span class="ne">StopIteration</span><span class="p">()</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;putting job #</span><span class="si">%i</span><span class="s2"> in the queue&quot;</span><span class="p">,</span> <span class="n">job_no</span><span class="p">)</span>
                <span class="n">job_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">items</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;reached end of input; waiting to finish </span><span class="si">%i</span><span class="s2"> outstanding jobs&quot;</span><span class="p">,</span>
                    <span class="n">job_no</span> <span class="o">-</span> <span class="n">done_jobs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">workers</span><span class="p">):</span>
                    <span class="n">job_queue</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>  <span class="c1"># give the workers heads up that they can finish -- no more work!</span>
                <span class="n">push_done</span> <span class="o">=</span> <span class="bp">True</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">while</span> <span class="n">done_jobs</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">job_no</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">push_done</span><span class="p">:</span>
                    <span class="n">ns</span> <span class="o">=</span> <span class="n">progress_queue</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">push_done</span><span class="p">)</span>  <span class="c1"># only block after all jobs pushed</span>
                    <span class="n">sentence_count</span> <span class="o">+=</span> <span class="n">ns</span>
                    <span class="n">done_jobs</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="n">elapsed</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
                    <span class="k">if</span> <span class="n">elapsed</span> <span class="o">&gt;=</span> <span class="n">next_report</span><span class="p">:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                            <span class="s2">&quot;PROGRESS: at </span><span class="si">%.2f%%</span><span class="s2"> sentences, </span><span class="si">%.0f</span><span class="s2"> sentences/s&quot;</span><span class="p">,</span>
                            <span class="mf">100.0</span> <span class="o">*</span> <span class="n">sentence_count</span><span class="p">,</span> <span class="n">sentence_count</span> <span class="o">/</span> <span class="n">elapsed</span><span class="p">)</span>
                        <span class="n">next_report</span> <span class="o">=</span> <span class="n">elapsed</span> <span class="o">+</span> <span class="n">report_delay</span>  <span class="c1"># don&#39;t flood log, wait report_delay seconds</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># loop ended by job count; really done</span>
                    <span class="k">break</span>
            <span class="k">except</span> <span class="n">Empty</span><span class="p">:</span>
                <span class="k">pass</span>  <span class="c1"># already out of loop; continue to next push</span>

        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">default_timer</span><span class="p">()</span> <span class="o">-</span> <span class="n">start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clear_sims</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;scoring </span><span class="si">%i</span><span class="s2"> sentences took </span><span class="si">%.1f</span><span class="s2">s, </span><span class="si">%.0f</span><span class="s2"> sentences/s&quot;</span><span class="p">,</span>
            <span class="n">sentence_count</span><span class="p">,</span> <span class="n">elapsed</span><span class="p">,</span> <span class="n">sentence_count</span> <span class="o">/</span> <span class="n">elapsed</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sentence_scores</span><span class="p">[:</span><span class="n">sentence_count</span><span class="p">]</span></div>

<div class="viewcode-block" id="Word2Vec.clear_sims"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.clear_sims.html#gensim.models.Word2Vec.clear_sims">[docs]</a>    <span class="k">def</span> <span class="nf">clear_sims</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span> <span class="o">=</span> <span class="bp">None</span></div>

<div class="viewcode-block" id="Word2Vec.reset_weights"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.reset_weights.html#gensim.models.Word2Vec.reset_weights">[docs]</a>    <span class="k">def</span> <span class="nf">reset_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.&quot;&quot;&quot;</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;resetting layer weights&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span> <span class="o">=</span> <span class="n">empty</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
        <span class="c1"># randomize weights vector by vector, rather than materializing a huge random matrix in RAM at once</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)):</span>
            <span class="c1"># construct deterministic seed from word AND seed argument</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seeded_vector</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">syn1</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">syn1neg</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">syn0_lockf</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>  <span class="c1"># zeros suppress learning</span></div>

<div class="viewcode-block" id="Word2Vec.seeded_vector"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.seeded_vector.html#gensim.models.Word2Vec.seeded_vector">[docs]</a>    <span class="k">def</span> <span class="nf">seeded_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed_string</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create one &#39;random&#39; vector (but deterministic by seed_string)&quot;&quot;&quot;</span>
        <span class="c1"># Note: built-in hash() may vary by Python version or even (in Py3.x) per launch</span>
        <span class="n">once</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hashfxn</span><span class="p">(</span><span class="n">seed_string</span><span class="p">)</span> <span class="o">&amp;</span> <span class="mh">0xffffffff</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">once</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span></div>

<div class="viewcode-block" id="Word2Vec.save_word2vec_format"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.save_word2vec_format.html#gensim.models.Word2Vec.save_word2vec_format">[docs]</a>    <span class="k">def</span> <span class="nf">save_word2vec_format</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">fvocab</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Store the input-hidden weight matrix in the same format used by the original</span>
<span class="sd">        C word2vec-tool, for compatibility.</span>

<span class="sd">         `fname` is the file used to save the vectors in</span>
<span class="sd">         `fvocab` is an optional file used to save the vocabulary</span>
<span class="sd">         `binary` is an optional boolean indicating whether the data is to be saved</span>
<span class="sd">         in binary word2vec format (default: False)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">fvocab</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;storing vocabulary in </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fvocab</span><span class="p">))</span>
            <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="n">fvocab</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">vout</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">vocab</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="o">-</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">):</span>
                    <span class="n">vout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_utf8</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">vocab</span><span class="o">.</span><span class="n">count</span><span class="p">)))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;storing </span><span class="si">%s</span><span class="s2">x</span><span class="si">%s</span><span class="s2"> projection weights into </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">,</span> <span class="n">fname</span><span class="p">))</span>
        <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">)</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fout</span><span class="p">:</span>
            <span class="n">fout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_utf8</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
            <span class="c1"># store in sorted order: most frequent words at the top</span>
            <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">vocab</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">iteritems</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">item</span><span class="p">:</span> <span class="o">-</span><span class="n">item</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">):</span>
                <span class="n">row</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">vocab</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">binary</span><span class="p">:</span>
                    <span class="n">fout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_utf8</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">row</span><span class="o">.</span><span class="n">tostring</span><span class="p">())</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">fout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_utf8</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> </span><span class="si">%s</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">val</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">row</span><span class="p">))))</span></div>

    <span class="nd">@classmethod</span>
<div class="viewcode-block" id="Word2Vec.load_word2vec_format"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.load_word2vec_format.html#gensim.models.Word2Vec.load_word2vec_format">[docs]</a>    <span class="k">def</span> <span class="nf">load_word2vec_format</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">fvocab</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf8&#39;</span><span class="p">,</span> <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;strict&#39;</span><span class="p">,</span>
                             <span class="n">limit</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">datatype</span><span class="o">=</span><span class="n">REAL</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load the input-hidden weight matrix from the original C word2vec-tool format.</span>

<span class="sd">        Note that the information stored in the file is incomplete (the binary tree is missing),</span>
<span class="sd">        so while you can query for word similarity etc., you cannot continue training</span>
<span class="sd">        with a model loaded this way.</span>

<span class="sd">        `binary` is a boolean indicating whether the data is in binary word2vec format.</span>
<span class="sd">        `norm_only` is a boolean indicating whether to only store normalised word2vec vectors in memory.</span>
<span class="sd">        Word counts are read from `fvocab` filename, if set (this is the file generated</span>
<span class="sd">        by `-save-vocab` flag of the original C tool).</span>

<span class="sd">        If you trained the C model using non-utf8 encoding for words, specify that</span>
<span class="sd">        encoding in `encoding`.</span>

<span class="sd">        `unicode_errors`, default &#39;strict&#39;, is a string suitable to be passed as the `errors`</span>
<span class="sd">        argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source</span>
<span class="sd">        file may include word tokens truncated in the middle of a multibyte unicode character</span>
<span class="sd">        (as is common from the original word2vec.c tool), &#39;ignore&#39; or &#39;replace&#39; may help.</span>

<span class="sd">        `limit` sets a maximum number of word-vectors to read from the file. The default,</span>
<span class="sd">        None, means read all.</span>

<span class="sd">        `datatype` (experimental) can coerce dimensions to a non-default float type (such</span>
<span class="sd">        as np.float16) to save memory. (Such types may result in much slower bulk operations</span>
<span class="sd">        or incompatibility with optimized routines.)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">counts</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">fvocab</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading word counts from </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">fvocab</span><span class="p">)</span>
            <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="n">fvocab</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">fin</span><span class="p">:</span>
                    <span class="n">word</span><span class="p">,</span> <span class="n">count</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
                    <span class="n">counts</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">count</span><span class="p">)</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading projection weights from </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
            <span class="n">header</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">fin</span><span class="o">.</span><span class="n">readline</span><span class="p">(),</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">)</span>
            <span class="n">vocab_size</span><span class="p">,</span> <span class="n">vector_size</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">header</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>  <span class="c1"># throws for invalid file format</span>
            <span class="k">if</span> <span class="n">limit</span><span class="p">:</span>
                <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">limit</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">cls</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">vector_size</span><span class="p">)</span>
            <span class="n">result</span><span class="o">.</span><span class="n">syn0</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vector_size</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">datatype</span><span class="p">)</span>

            <span class="k">def</span> <span class="nf">add_word</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
                <span class="n">word_id</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">result</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;duplicate word &#39;</span><span class="si">%s</span><span class="s2">&#39; in </span><span class="si">%s</span><span class="s2">, ignoring all but first&quot;</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span>
                    <span class="k">return</span>
                <span class="k">if</span> <span class="n">counts</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                    <span class="c1"># most common scenario: no vocab file given. just make up some bogus counts, in descending order</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">word_id</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="n">word_id</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">counts</span><span class="p">:</span>
                    <span class="c1"># use count from the vocab file</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">word_id</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="n">counts</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># vocab file given, but word is missing -- set count to None (TODO: or raise?)</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;vocabulary file is incomplete: &#39;</span><span class="si">%s</span><span class="s2">&#39; is missing&quot;</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span>
                    <span class="n">result</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">Vocab</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">word_id</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
                <span class="n">result</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>
                <span class="n">result</span><span class="o">.</span><span class="n">index2word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">binary</span><span class="p">:</span>
                <span class="n">binary_len</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span> <span class="o">*</span> <span class="n">vector_size</span>
                <span class="k">for</span> <span class="n">line_no</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
                    <span class="c1"># mixed text and binary: read text first, then binary</span>
                    <span class="n">word</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
                        <span class="n">ch</span> <span class="o">=</span> <span class="n">fin</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">ch</span> <span class="o">==</span> <span class="n">b</span><span class="s1">&#39; &#39;</span><span class="p">:</span>
                            <span class="k">break</span>
                        <span class="k">if</span> <span class="n">ch</span> <span class="o">==</span> <span class="n">b</span><span class="s1">&#39;&#39;</span><span class="p">:</span>
                            <span class="k">raise</span> <span class="ne">EOFError</span><span class="p">(</span><span class="s2">&quot;unexpected end of input; is count incorrect or file otherwise damaged?&quot;</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">ch</span> <span class="o">!=</span> <span class="n">b</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">:</span>  <span class="c1"># ignore newlines in front of words (some binary files have)</span>
                            <span class="n">word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ch</span><span class="p">)</span>
                    <span class="n">word</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">b</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="n">unicode_errors</span><span class="p">)</span>
                    <span class="n">weights</span> <span class="o">=</span> <span class="n">fromstring</span><span class="p">(</span><span class="n">fin</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">binary_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
                    <span class="n">add_word</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">line_no</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
                    <span class="n">line</span> <span class="o">=</span> <span class="n">fin</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">line</span> <span class="o">==</span> <span class="n">b</span><span class="s1">&#39;&#39;</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">EOFError</span><span class="p">(</span><span class="s2">&quot;unexpected end of input; is count incorrect or file otherwise damaged?&quot;</span><span class="p">)</span>
                    <span class="n">parts</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(),</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="n">unicode_errors</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span> <span class="o">!=</span> <span class="n">vector_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;invalid vector on line </span><span class="si">%s</span><span class="s2"> (is this really the text format?)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">line_no</span><span class="p">))</span>
                    <span class="n">word</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">REAL</span><span class="p">,</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
                    <span class="n">add_word</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">syn0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">vocab</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;duplicate words detected, shrinking matrix size from </span><span class="si">%i</span><span class="s2"> to </span><span class="si">%i</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">result</span><span class="o">.</span><span class="n">syn0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">result</span><span class="o">.</span><span class="n">syn0</span> <span class="o">=</span> <span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">syn0</span><span class="p">[:</span> <span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">vocab</span><span class="p">)])</span>
        <span class="k">assert</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">result</span><span class="o">.</span><span class="n">vector_size</span><span class="p">)</span> <span class="o">==</span> <span class="n">result</span><span class="o">.</span><span class="n">syn0</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loaded </span><span class="si">%s</span><span class="s2"> matrix from </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">syn0</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">fname</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">result</span></div>

<div class="viewcode-block" id="Word2Vec.intersect_word2vec_format"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.intersect_word2vec_format.html#gensim.models.Word2Vec.intersect_word2vec_format">[docs]</a>    <span class="k">def</span> <span class="nf">intersect_word2vec_format</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">lockf</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf8&#39;</span><span class="p">,</span> <span class="n">unicode_errors</span><span class="o">=</span><span class="s1">&#39;strict&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Merge the input-hidden weight matrix from the original C word2vec-tool format</span>
<span class="sd">        given, where it intersects with the current vocabulary. (No words are added to the</span>
<span class="sd">        existing vocabulary, but intersecting words adopt the file&#39;s weights, and</span>
<span class="sd">        non-intersecting words are left alone.)</span>

<span class="sd">        `binary` is a boolean indicating whether the data is in binary word2vec format.</span>

<span class="sd">        `lockf` is a lock-factor value to be set for any imported word-vectors; the</span>
<span class="sd">        default value of 0.0 prevents further updating of the vector during subsequent</span>
<span class="sd">        training. Use 1.0 to allow further training updates of merged vectors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">overlap_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;loading projection weights from </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">fname</span><span class="p">))</span>
        <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
            <span class="n">header</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">fin</span><span class="o">.</span><span class="n">readline</span><span class="p">(),</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">)</span>
            <span class="n">vocab_size</span><span class="p">,</span> <span class="n">vector_size</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">header</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>  <span class="c1"># throws for invalid file format</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">vector_size</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;incompatible vector size </span><span class="si">%d</span><span class="s2"> in file </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">vector_size</span><span class="p">,</span> <span class="n">fname</span><span class="p">))</span>
                <span class="c1"># TOCONSIDER: maybe mismatched vectors still useful enough to merge (truncating/padding)?</span>
            <span class="k">if</span> <span class="n">binary</span><span class="p">:</span>
                <span class="n">binary_len</span> <span class="o">=</span> <span class="n">dtype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span> <span class="o">*</span> <span class="n">vector_size</span>
                <span class="k">for</span> <span class="n">line_no</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">):</span>
                    <span class="c1"># mixed text and binary: read text first, then binary</span>
                    <span class="n">word</span> <span class="o">=</span> <span class="p">[]</span>
                    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
                        <span class="n">ch</span> <span class="o">=</span> <span class="n">fin</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">ch</span> <span class="o">==</span> <span class="n">b</span><span class="s1">&#39; &#39;</span><span class="p">:</span>
                            <span class="k">break</span>
                        <span class="k">if</span> <span class="n">ch</span> <span class="o">!=</span> <span class="n">b</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">:</span>  <span class="c1"># ignore newlines in front of words (some binary files have)</span>
                            <span class="n">word</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ch</span><span class="p">)</span>
                    <span class="n">word</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">b</span><span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="n">unicode_errors</span><span class="p">)</span>
                    <span class="n">weights</span> <span class="o">=</span> <span class="n">fromstring</span><span class="p">(</span><span class="n">fin</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="n">binary_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
                        <span class="n">overlap_count</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">syn0_lockf</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">lockf</span>  <span class="c1"># lock-factor: 0.0 stops further changes</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">line_no</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">fin</span><span class="p">):</span>
                    <span class="n">parts</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(),</span> <span class="n">encoding</span><span class="o">=</span><span class="n">encoding</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="n">unicode_errors</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span> <span class="o">!=</span> <span class="n">vector_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;invalid vector on line </span><span class="si">%s</span><span class="s2"> (is this really the text format?)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">line_no</span><span class="p">))</span>
                    <span class="n">word</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">REAL</span><span class="p">,</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
                    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
                        <span class="n">overlap_count</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;merged </span><span class="si">%d</span><span class="s2"> vectors into </span><span class="si">%s</span><span class="s2"> matrix from </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">overlap_count</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">fname</span><span class="p">))</span></div>

<div class="viewcode-block" id="Word2Vec.most_similar"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.most_similar.html#gensim.models.Word2Vec.most_similar">[docs]</a>    <span class="k">def</span> <span class="nf">most_similar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">positive</span><span class="o">=</span><span class="p">[],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">indexer</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find the top-N most similar words. Positive words contribute positively towards the</span>
<span class="sd">        similarity, negative words negatively.</span>

<span class="sd">        This method computes cosine similarity between a simple mean of the projection</span>
<span class="sd">        weight vectors of the given words and the vectors for each word in the model.</span>
<span class="sd">        The method corresponds to the `word-analogy` and `distance` scripts in the original</span>
<span class="sd">        word2vec implementation.</span>

<span class="sd">        If topn is False, most_similar returns the vector of similarity scores.</span>

<span class="sd">        `restrict_vocab` is an optional integer which limits the range of vectors which</span>
<span class="sd">        are searched for most-similar values. For example, restrict_vocab=10000 would</span>
<span class="sd">        only check the first 10000 word vectors in the vocabulary order. (This may be</span>
<span class="sd">        meaningful if you&#39;ve sorted the vocabulary by descending frequency.)</span>

<span class="sd">        Example::</span>

<span class="sd">          &gt;&gt;&gt; trained_model.most_similar(positive=[&#39;woman&#39;, &#39;king&#39;], negative=[&#39;man&#39;])</span>
<span class="sd">          [(&#39;queen&#39;, 0.50882536), ...]</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_sims</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">positive</span><span class="p">,</span> <span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">negative</span><span class="p">:</span>
            <span class="c1"># allow calls like most_similar(&#39;dog&#39;), as a shorthand for most_similar([&#39;dog&#39;])</span>
            <span class="n">positive</span> <span class="o">=</span> <span class="p">[</span><span class="n">positive</span><span class="p">]</span>

        <span class="c1"># add weights for each word, if not already present; default to 1.0 for positive and -1.0 for negative words</span>
        <span class="n">positive</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">string_types</span> <span class="o">+</span> <span class="p">(</span><span class="n">ndarray</span><span class="p">,))</span> <span class="k">else</span> <span class="n">word</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">positive</span>
        <span class="p">]</span>
        <span class="n">negative</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">string_types</span> <span class="o">+</span> <span class="p">(</span><span class="n">ndarray</span><span class="p">,))</span> <span class="k">else</span> <span class="n">word</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">negative</span>
        <span class="p">]</span>

        <span class="c1"># compute the weighted average of all words</span>
        <span class="n">all_words</span><span class="p">,</span> <span class="n">mean</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(),</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">positive</span> <span class="o">+</span> <span class="n">negative</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">):</span>
                <span class="n">mean</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="n">word</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
                <span class="n">mean</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">])</span>
                <span class="n">all_words</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;word &#39;</span><span class="si">%s</span><span class="s2">&#39; not in vocabulary&quot;</span> <span class="o">%</span> <span class="n">word</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">mean</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;cannot compute similarity with no input&quot;</span><span class="p">)</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">unitvec</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">indexer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">indexer</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">topn</span><span class="p">)</span>

        <span class="n">limited</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span> <span class="k">if</span> <span class="n">restrict_vocab</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span><span class="p">[:</span><span class="n">restrict_vocab</span><span class="p">]</span>
        <span class="n">dists</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">limited</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">topn</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dists</span>
        <span class="n">best</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="n">topn</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_words</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># ignore (don&#39;t return) words from the input</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">sim</span><span class="p">],</span> <span class="nb">float</span><span class="p">(</span><span class="n">dists</span><span class="p">[</span><span class="n">sim</span><span class="p">]))</span> <span class="k">for</span> <span class="n">sim</span> <span class="ow">in</span> <span class="n">best</span> <span class="k">if</span> <span class="n">sim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">all_words</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">[:</span><span class="n">topn</span><span class="p">]</span></div>

<div class="viewcode-block" id="Word2Vec.wmdistance"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.wmdistance.html#gensim.models.Word2Vec.wmdistance">[docs]</a>    <span class="k">def</span> <span class="nf">wmdistance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">document1</span><span class="p">,</span> <span class="n">document2</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the Word Mover&#39;s Distance between two documents. When using this</span>
<span class="sd">        code, please consider citing the following papers:</span>

<span class="sd">        .. Ofir Pele and Michael Werman, &quot;A linear time histogram metric for improved SIFT matching&quot;.</span>
<span class="sd">        .. Ofir Pele and Michael Werman, &quot;Fast and robust earth mover&#39;s distances&quot;.</span>
<span class="sd">        .. Matt Kusner et al. &quot;From Word Embeddings To Document Distances&quot;.</span>

<span class="sd">        Note that if one of the documents have no words that exist in the</span>
<span class="sd">        Word2Vec vocab, `float(&#39;inf&#39;)` (i.e. infinity) will be returned.</span>

<span class="sd">        This method only works if `pyemd` is installed (can be installed via pip, but requires a C compiler).</span>

<span class="sd">        Example:</span>
<span class="sd">            &gt;&gt;&gt; # Train word2vec model.</span>
<span class="sd">            &gt;&gt;&gt; model = Word2Vec(sentences)</span>

<span class="sd">            &gt;&gt;&gt; # Some sentences to test.</span>
<span class="sd">            &gt;&gt;&gt; sentence_obama = &#39;Obama speaks to the media in Illinois&#39;.lower().split()</span>
<span class="sd">            &gt;&gt;&gt; sentence_president = &#39;The president greets the press in Chicago&#39;.lower().split()</span>

<span class="sd">            &gt;&gt;&gt; # Remove their stopwords.</span>
<span class="sd">            &gt;&gt;&gt; from nltk.corpus import stopwords</span>
<span class="sd">            &gt;&gt;&gt; stopwords = nltk.corpus.stopwords.words(&#39;english&#39;)</span>
<span class="sd">            &gt;&gt;&gt; sentence_obama = [w for w in sentence_obama if w not in stopwords]</span>
<span class="sd">            &gt;&gt;&gt; sentence_president = [w for w in sentence_president if w not in stopwords]</span>

<span class="sd">            &gt;&gt;&gt; # Compute WMD.</span>
<span class="sd">            &gt;&gt;&gt; distance = model.wmdistance(sentence_obama, sentence_president)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">PYEMD_EXT</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Please install pyemd Python package to compute WMD.&quot;</span><span class="p">)</span>

        <span class="c1"># Remove out-of-vocabulary words.</span>
        <span class="n">len_pre_oov1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">document1</span><span class="p">)</span>
        <span class="n">len_pre_oov2</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">document2</span><span class="p">)</span>
        <span class="n">document1</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">document1</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">]</span>
        <span class="n">document2</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">document2</span> <span class="k">if</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">]</span>
        <span class="n">diff1</span> <span class="o">=</span> <span class="n">len_pre_oov1</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">document1</span><span class="p">)</span>
        <span class="n">diff2</span> <span class="o">=</span> <span class="n">len_pre_oov2</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">document2</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">diff1</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">diff2</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Removed </span><span class="si">%d</span><span class="s1"> and </span><span class="si">%d</span><span class="s1"> OOV words from document 1 and 2 (respectively).&#39;</span><span class="p">,</span>
                        <span class="n">diff1</span><span class="p">,</span> <span class="n">diff2</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">document1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">document2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;At least one of the documents had no words that were&#39;</span>
                        <span class="s1">&#39;in the vocabulary. Aborting (returning inf).&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>

        <span class="n">dictionary</span> <span class="o">=</span> <span class="n">Dictionary</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="p">[</span><span class="n">document1</span><span class="p">,</span> <span class="n">document2</span><span class="p">])</span>
        <span class="n">vocab_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span>

        <span class="c1"># Sets for faster look-up.</span>
        <span class="n">docset1</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">document1</span><span class="p">)</span>
        <span class="n">docset2</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">document2</span><span class="p">)</span>

        <span class="c1"># Compute distance matrix.</span>
        <span class="n">distance_matrix</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">((</span><span class="n">vocab_len</span><span class="p">,</span> <span class="n">vocab_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">double</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t1</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">t2</span> <span class="ow">in</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">t1</span> <span class="ow">in</span> <span class="n">docset1</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">t2</span> <span class="ow">in</span> <span class="n">docset2</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="c1"># Compute Euclidean distance between word vectors.</span>
                <span class="n">distance_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">np_sum</span><span class="p">((</span><span class="bp">self</span><span class="p">[</span><span class="n">t1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="p">[</span><span class="n">t2</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">np_sum</span><span class="p">(</span><span class="n">distance_matrix</span><span class="p">)</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="c1"># `emd` gets stuck if the distance matrix contains only zeros.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;The distance matrix is all zeros. Aborting (returning inf).&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">nbow</span><span class="p">(</span><span class="n">document</span><span class="p">):</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="n">vocab_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">double</span><span class="p">)</span>
            <span class="n">nbow</span> <span class="o">=</span> <span class="n">dictionary</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>  <span class="c1"># Word frequencies.</span>
            <span class="n">doc_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="n">nbow</span><span class="p">:</span>
                <span class="n">d</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">freq</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">doc_len</span><span class="p">)</span>  <span class="c1"># Normalized word frequencies.</span>
            <span class="k">return</span> <span class="n">d</span>

        <span class="c1"># Compute nBOW representation of documents.</span>
        <span class="n">d1</span> <span class="o">=</span> <span class="n">nbow</span><span class="p">(</span><span class="n">document1</span><span class="p">)</span>
        <span class="n">d2</span> <span class="o">=</span> <span class="n">nbow</span><span class="p">(</span><span class="n">document2</span><span class="p">)</span>

        <span class="c1"># Compute WMD.</span>
        <span class="k">return</span> <span class="n">emd</span><span class="p">(</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">,</span> <span class="n">distance_matrix</span><span class="p">)</span></div>

<div class="viewcode-block" id="Word2Vec.most_similar_cosmul"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.most_similar_cosmul.html#gensim.models.Word2Vec.most_similar_cosmul">[docs]</a>    <span class="k">def</span> <span class="nf">most_similar_cosmul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">positive</span><span class="o">=</span><span class="p">[],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find the top-N most similar words, using the multiplicative combination objective</span>
<span class="sd">        proposed by Omer Levy and Yoav Goldberg in [4]_. Positive words still contribute</span>
<span class="sd">        positively towards the similarity, negative words negatively, but with less</span>
<span class="sd">        susceptibility to one large distance dominating the calculation.</span>

<span class="sd">        In the common analogy-solving case, of two positive and one negative examples,</span>
<span class="sd">        this method is equivalent to the &quot;3CosMul&quot; objective (equation (4)) of Levy and Goldberg.</span>

<span class="sd">        Additional positive or negative examples contribute to the numerator or denominator,</span>
<span class="sd">        respectively – a potentially sensible but untested extension of the method. (With</span>
<span class="sd">        a single positive example, rankings will be the same as in the default most_similar.)</span>

<span class="sd">        Example::</span>

<span class="sd">          &gt;&gt;&gt; trained_model.most_similar_cosmul(positive=[&#39;baghdad&#39;, &#39;england&#39;], negative=[&#39;london&#39;])</span>
<span class="sd">          [(u&#39;iraq&#39;, 0.8488819003105164), ...]</span>

<span class="sd">        .. [4] Omer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representations, 2014.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_sims</span><span class="p">()</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">positive</span><span class="p">,</span> <span class="n">string_types</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">negative</span><span class="p">:</span>
            <span class="c1"># allow calls like most_similar_cosmul(&#39;dog&#39;), as a shorthand for most_similar_cosmul([&#39;dog&#39;])</span>
            <span class="n">positive</span> <span class="o">=</span> <span class="p">[</span><span class="n">positive</span><span class="p">]</span>

        <span class="n">all_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">word_vec</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">ndarray</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">word</span>
            <span class="k">elif</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">:</span>
                <span class="n">all_words</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;word &#39;</span><span class="si">%s</span><span class="s2">&#39; not in vocabulary&quot;</span> <span class="o">%</span> <span class="n">word</span><span class="p">)</span>

        <span class="n">positive</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_vec</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">positive</span><span class="p">]</span>
        <span class="n">negative</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_vec</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">negative</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">positive</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;cannot compute similarity with no input&quot;</span><span class="p">)</span>

        <span class="c1"># equation (4) of Levy &amp; Goldberg &quot;Linguistic Regularities...&quot;,</span>
        <span class="c1"># with distances shifted to [0,1] per footnote (7)</span>
        <span class="n">pos_dists</span> <span class="o">=</span> <span class="p">[((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span><span class="p">,</span> <span class="n">term</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">positive</span><span class="p">]</span>
        <span class="n">neg_dists</span> <span class="o">=</span> <span class="p">[((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span><span class="p">,</span> <span class="n">term</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">negative</span><span class="p">]</span>
        <span class="n">dists</span> <span class="o">=</span> <span class="n">prod</span><span class="p">(</span><span class="n">pos_dists</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">prod</span><span class="p">(</span><span class="n">neg_dists</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.000001</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">topn</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dists</span>
        <span class="n">best</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="n">topn</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_words</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># ignore (don&#39;t return) words from the input</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">sim</span><span class="p">],</span> <span class="nb">float</span><span class="p">(</span><span class="n">dists</span><span class="p">[</span><span class="n">sim</span><span class="p">]))</span> <span class="k">for</span> <span class="n">sim</span> <span class="ow">in</span> <span class="n">best</span> <span class="k">if</span> <span class="n">sim</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">all_words</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">result</span><span class="p">[:</span><span class="n">topn</span><span class="p">]</span></div>

<div class="viewcode-block" id="Word2Vec.similar_by_word"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.similar_by_word.html#gensim.models.Word2Vec.similar_by_word">[docs]</a>    <span class="k">def</span> <span class="nf">similar_by_word</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find the top-N most similar words.</span>

<span class="sd">        If topn is False, similar_by_word returns the vector of similarity scores.</span>

<span class="sd">        `restrict_vocab` is an optional integer which limits the range of vectors which</span>
<span class="sd">        are searched for most-similar values. For example, restrict_vocab=10000 would</span>
<span class="sd">        only check the first 10000 word vectors in the vocabulary order. (This may be</span>
<span class="sd">        meaningful if you&#39;ve sorted the vocabulary by descending frequency.)</span>

<span class="sd">        Example::</span>

<span class="sd">          &gt;&gt;&gt; trained_model.similar_by_word(&#39;graph&#39;)</span>
<span class="sd">          [(&#39;user&#39;, 0.9999163150787354), ...]</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="n">word</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="n">topn</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="n">restrict_vocab</span><span class="p">)</span></div>

<div class="viewcode-block" id="Word2Vec.similar_by_vector"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.similar_by_vector.html#gensim.models.Word2Vec.similar_by_vector">[docs]</a>    <span class="k">def</span> <span class="nf">similar_by_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vector</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Find the top-N most similar words by vector.</span>

<span class="sd">        If topn is False, similar_by_vector returns the vector of similarity scores.</span>

<span class="sd">        `restrict_vocab` is an optional integer which limits the range of vectors which</span>
<span class="sd">        are searched for most-similar values. For example, restrict_vocab=10000 would</span>
<span class="sd">        only check the first 10000 word vectors in the vocabulary order. (This may be</span>
<span class="sd">        meaningful if you&#39;ve sorted the vocabulary by descending frequency.)</span>

<span class="sd">        Example::</span>

<span class="sd">          &gt;&gt;&gt; trained_model.similar_by_vector([1,2])</span>
<span class="sd">          [(&#39;survey&#39;, 0.9942699074745178), ...]</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="n">vector</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="n">topn</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="n">restrict_vocab</span><span class="p">)</span></div>

<div class="viewcode-block" id="Word2Vec.doesnt_match"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.doesnt_match.html#gensim.models.Word2Vec.doesnt_match">[docs]</a>    <span class="k">def</span> <span class="nf">doesnt_match</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Which word from the given list doesn&#39;t go with the others?</span>

<span class="sd">        Example::</span>

<span class="sd">          &gt;&gt;&gt; trained_model.doesnt_match(&quot;breakfast cereal dinner lunch&quot;.split())</span>
<span class="sd">          &#39;cereal&#39;</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_sims</span><span class="p">()</span>

        <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">]</span>  <span class="c1"># filter out OOV words</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;using words </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">words</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">words</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;cannot select a word from an empty list&quot;</span><span class="p">)</span>
        <span class="n">vectors</span> <span class="o">=</span> <span class="n">vstack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">matutils</span><span class="o">.</span><span class="n">unitvec</span><span class="p">(</span><span class="n">vectors</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span>
        <span class="n">dists</span> <span class="o">=</span> <span class="n">dot</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">words</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span></div>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Accept a single word or a list of words as input.</span>

<span class="sd">        If a single word: returns the word&#39;s representations in vector space, as</span>
<span class="sd">        a 1D numpy array.</span>

<span class="sd">        Multiple words: return the words&#39; representations in vector space, as a</span>
<span class="sd">        2d numpy array: #words x #vector_size. Matrix rows are in the same order</span>
<span class="sd">        as in input.</span>

<span class="sd">        Example::</span>

<span class="sd">          &gt;&gt;&gt; trained_model[&#39;office&#39;]</span>
<span class="sd">          array([ -1.40128313e-02, ...])</span>

<span class="sd">          &gt;&gt;&gt; trained_model[[&#39;office&#39;, &#39;products&#39;]]</span>
<span class="sd">          array([ -1.40128313e-02, ...]</span>
<span class="sd">                [ -1.70425311e-03, ...]</span>
<span class="sd">                 ...)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">string_types</span><span class="p">):</span>
            <span class="c1"># allow calls like trained_model[&#39;office&#39;], as a shorthand for trained_model[[&#39;office&#39;]]</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">words</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span>

<div class="viewcode-block" id="Word2Vec.similarity"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.similarity.html#gensim.models.Word2Vec.similarity">[docs]</a>    <span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute cosine similarity between two words.</span>

<span class="sd">        Example::</span>

<span class="sd">          &gt;&gt;&gt; trained_model.similarity(&#39;woman&#39;, &#39;man&#39;)</span>
<span class="sd">          0.73723527</span>

<span class="sd">          &gt;&gt;&gt; trained_model.similarity(&#39;woman&#39;, &#39;woman&#39;)</span>
<span class="sd">          1.0</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">matutils</span><span class="o">.</span><span class="n">unitvec</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">w1</span><span class="p">]),</span> <span class="n">matutils</span><span class="o">.</span><span class="n">unitvec</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">w2</span><span class="p">]))</span></div>

<div class="viewcode-block" id="Word2Vec.n_similarity"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.n_similarity.html#gensim.models.Word2Vec.n_similarity">[docs]</a>    <span class="k">def</span> <span class="nf">n_similarity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ws1</span><span class="p">,</span> <span class="n">ws2</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute cosine similarity between two sets of words.</span>

<span class="sd">        Example::</span>

<span class="sd">          &gt;&gt;&gt; trained_model.n_similarity([&#39;sushi&#39;, &#39;shop&#39;], [&#39;japanese&#39;, &#39;restaurant&#39;])</span>
<span class="sd">          0.61540466561049689</span>

<span class="sd">          &gt;&gt;&gt; trained_model.n_similarity([&#39;restaurant&#39;, &#39;japanese&#39;], [&#39;japanese&#39;, &#39;restaurant&#39;])</span>
<span class="sd">          1.0000000000000004</span>

<span class="sd">          &gt;&gt;&gt; trained_model.n_similarity([&#39;sushi&#39;], [&#39;restaurant&#39;]) == trained_model.similarity(&#39;sushi&#39;, &#39;restaurant&#39;)</span>
<span class="sd">          True</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">v1</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">ws1</span><span class="p">]</span>
        <span class="n">v2</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">ws2</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">matutils</span><span class="o">.</span><span class="n">unitvec</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="n">v1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)),</span> <span class="n">matutils</span><span class="o">.</span><span class="n">unitvec</span><span class="p">(</span><span class="n">array</span><span class="p">(</span><span class="n">v2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)))</span></div>

<div class="viewcode-block" id="Word2Vec.init_sims"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.init_sims.html#gensim.models.Word2Vec.init_sims">[docs]</a>    <span class="k">def</span> <span class="nf">init_sims</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Precompute L2-normalized vectors.</span>

<span class="sd">        If `replace` is set, forget the original vectors and only keep the normalized</span>
<span class="sd">        ones = saves lots of memory!</span>

<span class="sd">        Note that you **cannot continue training** after doing a replace. The model becomes</span>
<span class="sd">        effectively read-only = you can call `most_similar`, `similarity` etc., but not `train`.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;syn0norm&#39;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">replace</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;precomputing L2-norms of word weight vectors&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">replace</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">/=</span> <span class="n">sqrt</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">syn0</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn0</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;syn1&#39;</span><span class="p">):</span>
                    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">syn1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">syn0norm</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">syn0</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">syn0</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span></div>

<div class="viewcode-block" id="Word2Vec.estimate_memory"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.estimate_memory.html#gensim.models.Word2Vec.estimate_memory">[docs]</a>    <span class="k">def</span> <span class="nf">estimate_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">report</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Estimate required memory for a model using current settings and provided vocabulary size.&quot;&quot;&quot;</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
        <span class="n">report</span> <span class="o">=</span> <span class="n">report</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="n">report</span><span class="p">[</span><span class="s1">&#39;vocab&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="p">(</span><span class="mi">700</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span> <span class="k">else</span> <span class="mi">500</span><span class="p">)</span>
        <span class="n">report</span><span class="p">[</span><span class="s1">&#39;syn0&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span> <span class="o">*</span> <span class="n">dtype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hs</span><span class="p">:</span>
            <span class="n">report</span><span class="p">[</span><span class="s1">&#39;syn1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span> <span class="o">*</span> <span class="n">dtype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">negative</span><span class="p">:</span>
            <span class="n">report</span><span class="p">[</span><span class="s1">&#39;syn1neg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer1_size</span> <span class="o">*</span> <span class="n">dtype</span><span class="p">(</span><span class="n">REAL</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span>
        <span class="n">report</span><span class="p">[</span><span class="s1">&#39;total&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">report</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;estimated required memory for </span><span class="si">%i</span><span class="s2"> words and </span><span class="si">%i</span><span class="s2"> dimensions: </span><span class="si">%i</span><span class="s2"> bytes&quot;</span><span class="p">,</span>
                    <span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">,</span> <span class="n">report</span><span class="p">[</span><span class="s1">&#39;total&#39;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">report</span></div>

    <span class="nd">@staticmethod</span>
<div class="viewcode-block" id="Word2Vec.log_accuracy"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.log_accuracy.html#gensim.models.Word2Vec.log_accuracy">[docs]</a>    <span class="k">def</span> <span class="nf">log_accuracy</span><span class="p">(</span><span class="n">section</span><span class="p">):</span>
        <span class="n">correct</span><span class="p">,</span> <span class="n">incorrect</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">section</span><span class="p">[</span><span class="s1">&#39;correct&#39;</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">section</span><span class="p">[</span><span class="s1">&#39;incorrect&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">correct</span> <span class="o">+</span> <span class="n">incorrect</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: </span><span class="si">%.1f%%</span><span class="s2"> (</span><span class="si">%i</span><span class="s2">/</span><span class="si">%i</span><span class="s2">)&quot;</span> <span class="o">%</span>
                        <span class="p">(</span><span class="n">section</span><span class="p">[</span><span class="s1">&#39;section&#39;</span><span class="p">],</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="p">(</span><span class="n">correct</span> <span class="o">+</span> <span class="n">incorrect</span><span class="p">),</span>
                         <span class="n">correct</span><span class="p">,</span> <span class="n">correct</span> <span class="o">+</span> <span class="n">incorrect</span><span class="p">))</span></div>

<div class="viewcode-block" id="Word2Vec.accuracy"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.accuracy.html#gensim.models.Word2Vec.accuracy">[docs]</a>    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">questions</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">most_similar</span><span class="o">=</span><span class="n">most_similar</span><span class="p">,</span> <span class="n">case_insensitive</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute accuracy of the model. `questions` is a filename where lines are</span>
<span class="sd">        4-tuples of words, split into sections by &quot;: SECTION NAME&quot; lines.</span>
<span class="sd">        See questions-words.txt in https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip for an example.</span>

<span class="sd">        The accuracy is reported (=printed to log and returned as a list) for each</span>
<span class="sd">        section separately, plus there&#39;s one aggregate summary at the end.</span>

<span class="sd">        Use `restrict_vocab` to ignore all questions containing a word not in the first `restrict_vocab`</span>
<span class="sd">        words (default 30,000). This may be meaningful if you&#39;ve sorted the vocabulary by descending frequency.</span>
<span class="sd">        In case `case_insensitive` is True, the first `restrict_vocab` words are taken first, and then</span>
<span class="sd">        case normalization is performed.</span>

<span class="sd">        Use `case_insensitive` to convert all words in questions and vocab to their uppercase form before </span>
<span class="sd">        evaluating the accuracy (default True). Useful in case of case-mismatch between training tokens </span>
<span class="sd">        and question words. In case of multiple case variants of a single word, the vector for the first</span>
<span class="sd">        occurrence (also the most frequent if vocabulary is sorted) is taken.</span>

<span class="sd">        This method corresponds to the `compute-accuracy` script of the original C word2vec.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ok_vocab</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">w</span><span class="p">])</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[:</span><span class="n">restrict_vocab</span><span class="p">]]</span>
        <span class="n">ok_vocab</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">w</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">ok_vocab</span><span class="p">))</span> <span class="k">if</span> <span class="n">case_insensitive</span> <span class="k">else</span> <span class="nb">dict</span><span class="p">(</span><span class="n">ok_vocab</span><span class="p">)</span>

        <span class="n">sections</span><span class="p">,</span> <span class="n">section</span> <span class="o">=</span> <span class="p">[],</span> <span class="bp">None</span>
        <span class="k">for</span> <span class="n">line_no</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="n">questions</span><span class="p">)):</span>
            <span class="c1"># TODO: use level3 BLAS (=evaluate multiple questions at once), for speed</span>
            <span class="n">line</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;: &#39;</span><span class="p">):</span>
                <span class="c1"># a new section starts =&gt; store the old section</span>
                <span class="k">if</span> <span class="n">section</span><span class="p">:</span>
                    <span class="n">sections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">section</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">log_accuracy</span><span class="p">(</span><span class="n">section</span><span class="p">)</span>
                <span class="n">section</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;section&#39;</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">lstrip</span><span class="p">(</span><span class="s1">&#39;: &#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="s1">&#39;correct&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;incorrect&#39;</span><span class="p">:</span> <span class="p">[]}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">section</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;missing section header before line #</span><span class="si">%i</span><span class="s2"> in </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">line_no</span><span class="p">,</span> <span class="n">questions</span><span class="p">))</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">case_insensitive</span><span class="p">:</span>
                        <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()]</span>
                <span class="k">except</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;skipping invalid line #</span><span class="si">%i</span><span class="s2"> in </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">line_no</span><span class="p">,</span> <span class="n">questions</span><span class="p">))</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">a</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ok_vocab</span> <span class="ow">or</span> <span class="n">b</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ok_vocab</span> <span class="ow">or</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ok_vocab</span> <span class="ow">or</span> <span class="n">expected</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ok_vocab</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;skipping line #</span><span class="si">%i</span><span class="s2"> with OOV words: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">line_no</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()))</span>
                    <span class="k">continue</span>

                <span class="n">original_vocab</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">ok_vocab</span>
                <span class="n">ignore</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span>  <span class="c1"># input words to be ignored</span>
                <span class="n">predicted</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="c1"># find the most likely prediction, ignoring OOV words and input words</span>
                <span class="n">sims</span> <span class="o">=</span> <span class="n">most_similar</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">restrict_vocab</span><span class="o">=</span><span class="n">restrict_vocab</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">original_vocab</span>
                <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">matutils</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">sims</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
                    <span class="n">predicted</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="k">if</span> <span class="n">case_insensitive</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">predicted</span> <span class="ow">in</span> <span class="n">ok_vocab</span> <span class="ow">and</span> <span class="n">predicted</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ignore</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">predicted</span> <span class="o">!=</span> <span class="n">expected</span><span class="p">:</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: expected </span><span class="si">%s</span><span class="s2">, predicted </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
                        <span class="k">break</span>
                <span class="k">if</span> <span class="n">predicted</span> <span class="o">==</span> <span class="n">expected</span><span class="p">:</span>
                    <span class="n">section</span><span class="p">[</span><span class="s1">&#39;correct&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">section</span><span class="p">[</span><span class="s1">&#39;incorrect&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">section</span><span class="p">:</span>
            <span class="c1"># store the last section, too</span>
            <span class="n">sections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">section</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_accuracy</span><span class="p">(</span><span class="n">section</span><span class="p">)</span>

        <span class="n">total</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;section&#39;</span><span class="p">:</span> <span class="s1">&#39;total&#39;</span><span class="p">,</span>
            <span class="s1">&#39;correct&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">((</span><span class="n">s</span><span class="p">[</span><span class="s1">&#39;correct&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sections</span><span class="p">),</span> <span class="p">[]),</span>
            <span class="s1">&#39;incorrect&#39;</span><span class="p">:</span> <span class="nb">sum</span><span class="p">((</span><span class="n">s</span><span class="p">[</span><span class="s1">&#39;incorrect&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sections</span><span class="p">),</span> <span class="p">[]),</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_accuracy</span><span class="p">(</span><span class="n">total</span><span class="p">)</span>
        <span class="n">sections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sections</span></div>

    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">(vocab=</span><span class="si">%s</span><span class="s2">, size=</span><span class="si">%s</span><span class="s2">, alpha=</span><span class="si">%s</span><span class="s2">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">index2word</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>

<div class="viewcode-block" id="Word2Vec.save"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.save.html#gensim.models.Word2Vec.save">[docs]</a>    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># don&#39;t bother storing the cached normalized vectors, recalculable table</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;ignore&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;syn0norm&#39;</span><span class="p">,</span> <span class="s1">&#39;table&#39;</span><span class="p">,</span> <span class="s1">&#39;cum_table&#39;</span><span class="p">])</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Word2Vec</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>

    <span class="n">save</span><span class="o">.</span><span class="n">__doc__</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">SaveLoad</span><span class="o">.</span><span class="n">save</span><span class="o">.</span><span class="n">__doc__</span>

    <span class="nd">@classmethod</span>
<div class="viewcode-block" id="Word2Vec.load"><a class="viewcode-back" href="../../../generated/generated/gensim.models.Word2Vec.load.html#gensim.models.Word2Vec.load">[docs]</a>    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">Word2Vec</span><span class="p">,</span> <span class="n">cls</span><span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># update older models</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;table&#39;</span><span class="p">):</span>
            <span class="nb">delattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;table&#39;</span><span class="p">)</span>  <span class="c1"># discard in favor of cum_table</span>
        <span class="k">if</span> <span class="n">model</span><span class="o">.</span><span class="n">negative</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;index2word&#39;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">make_cum_table</span><span class="p">()</span>  <span class="c1"># rebuild cum_table from vocabulary</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;corpus_count&#39;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s1">&#39;sample_int&#39;</span><span class="p">):</span>
                <span class="k">break</span>  <span class="c1"># already 0.12.0+ style int probabilities</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="s1">&#39;sample_probability&#39;</span><span class="p">):</span>
                <span class="n">v</span><span class="o">.</span><span class="n">sample_int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">sample_probability</span> <span class="o">*</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="p">))</span>
                <span class="k">del</span> <span class="n">v</span><span class="o">.</span><span class="n">sample_probability</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;syn0_lockf&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;syn0&#39;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">syn0_lockf</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">syn0</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">REAL</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">random</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;train_count&#39;</span><span class="p">):</span>
            <span class="n">model</span><span class="o">.</span><span class="n">train_count</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">model</span><span class="o">.</span><span class="n">total_train_time</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">model</span></div></div>


<span class="k">class</span> <span class="nc">BrownCorpus</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Iterate over sentences from the Brown corpus (part of NLTK data).&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dirname</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dirname</span> <span class="o">=</span> <span class="n">dirname</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">fname</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirname</span><span class="p">):</span>
            <span class="n">fname</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirname</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
                <span class="n">line</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
                <span class="c1"># each file line is a single sentence in the Brown corpus</span>
                <span class="c1"># each token is WORD/POS_TAG</span>
                <span class="n">token_tags</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">]</span>
                <span class="c1"># ignore words with non-alphabetic tags like &quot;,&quot;, &quot;!&quot; etc (punctuation, weird stuff)</span>
                <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">/</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="n">tag</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span> <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">token_tags</span> <span class="k">if</span> <span class="n">tag</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">words</span><span class="p">:</span>  <span class="c1"># don&#39;t bother sending out empty sentences</span>
                    <span class="k">continue</span>
                <span class="k">yield</span> <span class="n">words</span>


<span class="k">class</span> <span class="nc">Text8Corpus</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Iterate over sentences from the &quot;text8&quot; corpus, unzipped from http://mattmahoney.net/dc/text8.zip .&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fname</span><span class="p">,</span> <span class="n">max_sentence_length</span><span class="o">=</span><span class="n">MAX_WORDS_IN_BATCH</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fname</span> <span class="o">=</span> <span class="n">fname</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span> <span class="o">=</span> <span class="n">max_sentence_length</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># the entire corpus is one gigantic line -- there are no sentence marks at all</span>
        <span class="c1"># so just split the sequence of tokens arbitrarily: 1 sentence = 1000 tokens</span>
        <span class="n">sentence</span><span class="p">,</span> <span class="n">rest</span> <span class="o">=</span> <span class="p">[],</span> <span class="n">b</span><span class="s1">&#39;&#39;</span>
        <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fname</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
            <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
                <span class="n">text</span> <span class="o">=</span> <span class="n">rest</span> <span class="o">+</span> <span class="n">fin</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="mi">8192</span><span class="p">)</span>  <span class="c1"># avoid loading the entire file (=1 line) into RAM</span>
                <span class="k">if</span> <span class="n">text</span> <span class="o">==</span> <span class="n">rest</span><span class="p">:</span>  <span class="c1"># EOF</span>
                    <span class="n">words</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
                    <span class="n">sentence</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>  <span class="c1"># return the last chunk of words, too (may be shorter/longer)</span>
                    <span class="k">if</span> <span class="n">sentence</span><span class="p">:</span>
                        <span class="k">yield</span> <span class="n">sentence</span>
                    <span class="k">break</span>
                <span class="n">last_token</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">rfind</span><span class="p">(</span><span class="n">b</span><span class="s1">&#39; &#39;</span><span class="p">)</span>  <span class="c1"># last token may have been split in two... keep for next iteration</span>
                <span class="n">words</span><span class="p">,</span> <span class="n">rest</span> <span class="o">=</span> <span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">text</span><span class="p">[:</span><span class="n">last_token</span><span class="p">])</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span>
                               <span class="n">text</span><span class="p">[</span><span class="n">last_token</span><span class="p">:]</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span> <span class="k">if</span> <span class="n">last_token</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="p">([],</span> <span class="n">text</span><span class="p">)</span>
                <span class="n">sentence</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
                <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">:</span>
                    <span class="k">yield</span> <span class="n">sentence</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">]</span>
                    <span class="n">sentence</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">:]</span>


<span class="k">class</span> <span class="nc">LineSentence</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Simple format: one sentence = one line; words already preprocessed and separated by whitespace.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">max_sentence_length</span><span class="o">=</span><span class="n">MAX_WORDS_IN_BATCH</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        `source` can be either a string or a file object. Clip the file to the first</span>
<span class="sd">        `limit` lines (or no clipped if limit is None, the default).</span>

<span class="sd">        Example::</span>

<span class="sd">            sentences = LineSentence(&#39;myfile.txt&#39;)</span>

<span class="sd">        Or for compressed files::</span>

<span class="sd">            sentences = LineSentence(&#39;compressed_text.txt.bz2&#39;)</span>
<span class="sd">            sentences = LineSentence(&#39;compressed_text.txt.gz&#39;)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source</span> <span class="o">=</span> <span class="n">source</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span> <span class="o">=</span> <span class="n">max_sentence_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">limit</span> <span class="o">=</span> <span class="n">limit</span>

    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Iterate through the lines in the source.&quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Assume it is a file-like object and try treating it as such</span>
            <span class="c1"># Things that don&#39;t have seek will trigger an exception</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">islice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit</span><span class="p">):</span>
                <span class="n">line</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
                <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
                    <span class="k">yield</span> <span class="n">line</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">]</span>
                    <span class="n">i</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span>
        <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
            <span class="c1"># If it didn&#39;t work like a file, use it as a string filename</span>
            <span class="k">with</span> <span class="n">utils</span><span class="o">.</span><span class="n">smart_open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">)</span> <span class="k">as</span> <span class="n">fin</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">islice</span><span class="p">(</span><span class="n">fin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">limit</span><span class="p">):</span>
                    <span class="n">line</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">to_unicode</span><span class="p">(</span><span class="n">line</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
                    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="k">while</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="p">):</span>
                        <span class="k">yield</span> <span class="n">line</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span><span class="p">]</span>
                        <span class="n">i</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sentence_length</span>


<span class="c1"># Example: ./word2vec.py -train data.txt -output vec.txt -size 200 -window 5 -sample 1e-4 -negative 5 -hs 0 -binary 0 -cbow 1 -iter 3</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">argparse</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
        <span class="n">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(asctime)s</span><span class="s1"> : </span><span class="si">%(threadName)s</span><span class="s1"> : </span><span class="si">%(levelname)s</span><span class="s1"> : </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span>
        <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;running </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">))</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;using optimization </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">FAST_VERSION</span><span class="p">)</span>

    <span class="c1"># check and process cmdline input</span>
    <span class="n">program</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="nb">globals</span><span class="p">()[</span><span class="s1">&#39;__doc__&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="nb">locals</span><span class="p">())</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="kn">from</span> <span class="nn">gensim.models.word2vec</span> <span class="kn">import</span> <span class="n">Word2Vec</span>  <span class="c1"># avoid referencing __main__ in pickle</span>

    <span class="n">seterr</span><span class="p">(</span><span class="nb">all</span><span class="o">=</span><span class="s1">&#39;raise&#39;</span><span class="p">)</span>  <span class="c1"># don&#39;t ignore numpy errors</span>

    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">()</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-train&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use text data from file TRAIN to train the model&quot;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-output&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use file OUTPUT to save the resulting word vectors&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-window&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Set max skip length WINDOW between words; default is 5&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-size&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Set size of word vectors; default is 100&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-sample&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Set threshold for occurrence of words. Those that appear with higher frequency in the training data will be randomly down-sampled; default is 1e-3, useful range is (0, 1e-5)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-hs&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use Hierarchical Softmax; default is 0 (not used)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-negative&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of negative examples; default is 5, common values are 3 - 10 (0 = not used)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-threads&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use THREADS threads (default 12)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-iter&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Run more training iterations (default 5)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-min_count&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;This will discard words that appear less than MIN_COUNT times; default is 5&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-cbow&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use the continuous bag of words model; default is 1 (use 0 for skip-gram model)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-binary&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Save the resulting vectors in binary mode; default is 0 (off)&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;-accuracy&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use questions from file ACCURACY to evaluate the model&quot;</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">cbow</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">skipgram</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">skipgram</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">corpus</span> <span class="o">=</span> <span class="n">LineSentence</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">train</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span>
        <span class="n">corpus</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">min_count</span><span class="p">,</span> <span class="n">workers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">threads</span><span class="p">,</span>
        <span class="n">window</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">window</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">sample</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="n">skipgram</span><span class="p">,</span> <span class="n">hs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">hs</span><span class="p">,</span>
        <span class="n">negative</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">negative</span><span class="p">,</span> <span class="n">cbow_mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">iter</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">output</span><span class="p">:</span>
        <span class="n">outfile</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">output</span>
        <span class="n">model</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">outfile</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">binary</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">outfile</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">train</span>
        <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">outfile</span> <span class="o">+</span> <span class="s1">&#39;.model&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">binary</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">outfile</span> <span class="o">+</span> <span class="s1">&#39;.model.bin&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">save_word2vec_format</span><span class="p">(</span><span class="n">outfile</span> <span class="o">+</span> <span class="s1">&#39;.model.txt&#39;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">accuracy</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">accuracy</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;finished running </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">program</span><span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../_static/copybutton.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>